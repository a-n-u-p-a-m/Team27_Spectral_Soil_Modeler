# ðŸŒ¾ Spectral Soil Modeler - Comprehensive Documentation

## Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [ML Pipeline](#ml-pipeline)
4. [Models & Algorithms](#models--algorithms)
5. [Data Preprocessing](#data-preprocessing)
6. [Feature Engineering](#feature-engineering)
7. [Feature Importance Analysis](#feature-importance-analysis)
8. [Feature Implementation](#feature-implementation)
9. [Implementation Details](#implementation-details)
10. [Application Features](#application-features)
11. [Setup & Usage](#setup--usage)

---

## Project Overview

**Spectral Soil Modeler** is an advanced machine learning application for predicting soil properties using spectral data. It implements a complete ML pipeline with:

- **30 Model-Technique Combinations**: 5 algorithms Ã— 3 preprocessing techniques Ã— 2 paradigms
- **6 Feature Engineering Techniques**: Derivatives, Statistical, Polynomial, Spectral Indices, PCA, Wavelets
- **Dual Training Paradigms**: Standard vs Tuned hyperparameters (parallel execution)
- **Feature Importance Analysis**: Permutation-based ranking for ALL 30 combinations
- **Interactive Web UI**: Real-time training, prediction, and comprehensive analytics
- **AI-Powered Insights**: LLM integration with enhanced context (data analytics + feature engineering + hyperparameters + feature importance)
- **Comprehensive Analytics**: Multi-dimensional metrics, visualizations, and exports

### Key Statistics
- **5 ML Algorithms**: PLSR, GBRT, KRR, SVR, Cubist
- **3 Spectral Techniques**: Reflectance, Absorbance, Continuum Removal
- **6 Feature Engineering Methods**: Advanced spectral feature extraction
- **Dual Paradigm Training**: Standard + Tuned (parallel) = 30 total models
- **Feature Importance**: Computed for all 30 combinations
- **Cross-validation**: 5-fold for robust evaluation
- **AI Context**: Enhanced with data analytics, FE config, and feature importance

---

## Architecture

```
Project Structure
â”œâ”€â”€ soil_modeler/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app.py                    # Main Streamlit application with FE UI
â”‚   â”‚   â”œâ”€â”€ interface.py              # UI components & FE analytics tab
â”‚   â”‚   â”œâ”€â”€ services.py               # Enhanced context builder (FE + FI)
â”‚   â”‚   â”œâ”€â”€ preprocessing.py          # Spectral preprocessing + 6 FE methods
â”‚   â”‚   â”œâ”€â”€ data_loader.py            # Data loading & validation
â”‚   â”‚   â”œâ”€â”€ evaluation.py             # Model evaluation metrics
â”‚   â”‚   â”œâ”€â”€ model_analyzer.py         # Performance + Feature Importance
â”‚   â”‚   â”œâ”€â”€ chat_interface.py         # AI chat integration
â”‚   â”‚   â”œâ”€â”€ system.py                 # Logging, utilities + Excel export
â”‚   â”‚   â”œâ”€â”€ .env                      # API keys (Gemini/OpenAI)
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ model_trainer.py      # Training orchestration + FI
â”‚   â”‚       â”œâ”€â”€ plsr.py               # PLSR implementation
â”‚   â”‚       â”œâ”€â”€ gbrt.py               # GBRT implementation
â”‚   â”‚       â”œâ”€â”€ krr.py                # KRR implementation
â”‚   â”‚       â”œâ”€â”€ svr.py                # SVR implementation
â”‚   â”‚       â”œâ”€â”€ cubist.py             # Cubist implementation
â”‚   â”‚       â””â”€â”€ hyperparameter_tuner.py
â”‚   â”œâ”€â”€ requirements.txt              # Updated with pywt, seaborn, etc.
â”‚   â””â”€â”€ report.tex                    # LaTeX documentation
â””â”€â”€ README.md
```

### Application Flow Diagram

```
User Input (CSV Data)
    â†“
[Data Loading & Validation]
    â†“
[Data Analytics (Optional)]
â”œâ”€ Distribution analysis
â”œâ”€ Correlation heatmaps
â”œâ”€ Quality metrics
â””â”€ AI-powered data insights
    â†“
[Feature Engineering (Optional - Step 3.5)]
â”œâ”€ Spectral Derivatives (1st order)
â”œâ”€ Statistical Features (sliding window)
â”œâ”€ Polynomial Features (degree 2)
â”œâ”€ Spectral Indices (domain-specific)
â”œâ”€ PCA (dimensionality reduction)
â””â”€ Wavelet Features (multi-scale decomposition)
    â†“
[Preprocessing Pipeline]
â”œâ”€ Reflectance Transformation
â”œâ”€ Absorbance Transformation (Log-based)
â””â”€ Continuum Removal
    â†“
[Normalization: StandardScaler / MinMaxScaler / RobustScaler]
    â†“
[Training - Two Paradigms in Parallel]
â”œâ”€ Standard Pipeline (fixed hyperparameters) - 15 models
â””â”€ Tuned Pipeline (optimized hyperparameters) - 15 models
    â†“
[Feature Importance Computation]
â””â”€ Permutation importance for ALL 30 combinations
    â†“
[Model Evaluation & Cross-validation]
    â†“
[Results Display & Analysis]
â”œâ”€ Overview with KPIs
â”œâ”€ Analytics with filtering
â”œâ”€ Feature importance rankings
â”œâ”€ Comparison reports (Standard vs Tuned)
â”œâ”€ AI-powered recommendations (enhanced context)
â”œâ”€ Comprehensive Excel exports
â””â”€ Prediction interface
```

---

## ML Pipeline

### Phase 1: Data Preparation

```python
# Data Loading
data = DataLoader.load_from_csv("spectral_data.csv")
# Shape: (n_samples, n_features) where features are spectral bands

# Train-Test Split: 80-20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

### Phase 2: Preprocessing

The preprocessing pipeline applies:

1. **Spectral Transformations**:
   - **Reflectance**: `R = original_data` (baseline)
   - **Absorbance**: `A = log(1/R)` (emphasizes subtle features)
   - **Continuum Removal**: `CR = R / continuum(R)` (removes background)

2. **Normalization** (3 options):
   ```python
   StandardScaler()    # (X - mean) / std
   MinMaxScaler()      # (X - min) / (max - min)
   RobustScaler()      # (X - median) / IQR (robust to outliers)
   ```

3. **Optional Feature Engineering**:
   - Derivatives (first & second order)
   - Moving averages
   - Ratio calculations

### Phase 3: Training

**Standard Paradigm**: Fixed hyperparameters (defaults)
```python
models = {
    'PLSR': PLSRModel(n_components=10),
    'GBRT': GBRTModel(n_estimators=100, learning_rate=0.1, max_depth=5),
    'SVR': SVRModel(kernel='rbf', C=100.0, epsilon=0.1, gamma='scale'),
    'KRR': KRRModel(alpha=1.0, kernel='rbf', gamma=None),
    'Cubist': CubistModel(n_rules=20, neighbors=5)
}

# Results: 15 models (5 models Ã— 3 techniques) with default parameters
```

**Tuned Paradigm**: GridSearchCV / RandomizedSearchCV for optimization
```python
# Uses HyperparameterTuner with two approaches:
# 1. GridSearchCV: Exhaustive search (default)
# 2. RandomizedSearchCV: Random sampling (faster)

tuner = HyperparameterTuner(
    model_name='GBRT',
    cv_folds=5,                   # 5-fold cross-validation
    search_type='grid',           # 'grid' or 'random'
    use_small_grid=True,          # Use smaller grid for faster search (DEFAULT)
    n_iter=20                     # For RandomizedSearchCV only
)

best_params = tuner.tune_model(base_model, X_train, y_train)
# Returns: best_params, best_score, cv_results

# Results: 15 models (5 models Ã— 3 techniques) with OPTIMIZED parameters
```

### Phase 4: Evaluation

For each model:
- **Train Metrics**: RÂ², RMSE, MAE on training set
- **Test Metrics**: RÂ², RMSE, MAE on test set
- **Cross-validation**: 5-fold CV scores
- **RPD**: Residual Prediction Deviation

```python
metrics = {
    'Train_RÂ²': r2_score(y_train, y_pred_train),
    'Test_RÂ²': r2_score(y_test, y_pred_test),
    'Train_RMSE': sqrt(mse(y_train, y_pred_train)),
    'Test_RMSE': sqrt(mse(y_test, y_pred_test)),
    'Test_MAE': mean_absolute_error(y_test, y_pred_test),
    'RPD': std(y_test) / rmse(y_test, y_pred_test)
}
```

---

## Models & Algorithms

### 1. PLSR (Partial Least Squares Regression)

**Purpose**: Handle multicollinear spectral data by finding latent components

**Implementation**:
```python
class PLSRModel:
    def __init__(self, n_components=10):
        self.pls = PLSRegression(n_components=n_components)
    
    def train(self, X_train, y_train):
        return self.pls.fit(X_train, y_train)
    
    def predict(self, X):
        return self.pls.predict(X)
```

**Hyperparameters**:
- `n_components`: 5-15 (number of latent components)

**Best For**: Spectral data with high multicollinearity

---

### 2. GBRT (Gradient Boosting Regression Tree)

**Purpose**: Ensemble method combining weak learners sequentially

**Hyperparameters**:
- `n_estimators`: 50-300 (number of trees)
- `learning_rate`: 0.01-0.2 (shrinkage)
- `max_depth`: 3-10 (tree depth)

**Tuning Strategy**: Bayesian optimization on validation set

---

### 3. KRR (Kernel Ridge Regression)

**Purpose**: Non-linear regression with regularization

**Kernels Tested**:
- RBF (Radial Basis Function)
- Polynomial
- Linear

**Hyperparameters**:
- `alpha`: 0.01-1000 (regularization)
- `kernel`: rbf, poly, linear

---

### 4. SVR (Support Vector Regression)

**Purpose**: Support vector method for regression

**Hyperparameters**:
- `C`: 1-1000 (regularization parameter)
- `epsilon`: 0.01-0.5 (tube margin)
- `kernel`: rbf, poly, linear

---

### 5. Cubist

**Purpose**: Rule-based model generating interpretable rules

**Hyperparameters**:
- `n_rules`: 5-25 (number of rules to generate)
- `neighbors`: 0-9 (weighted neighbors for instance-based smoothing)

**Advantage**: Highly interpretable rule-based predictions

**Note**: Implemented as wrapper using tree-based regression (cubist.py)

---

## Data Preprocessing

### SpectralPreprocessor Class

```python
from preprocessing import SpectralPreprocessor

# Initialize
preprocessor = SpectralPreprocessor()

# Fit on training data
preprocessor.fit(
    X_train,
    technique='absorbance',      # spectral transformation
    scaler='standard',            # normalization
    apply_smoothing=False,
    smoothing_window=5
)

# Transform training and test data
X_train_processed = preprocessor.transform(X_train)
X_test_processed = preprocessor.transform(X_test)
```

### Spectral Transformations Explained

**1. Reflectance (Baseline)**
```
R = I_reflected / I_incident
Range: [0, 1]
Use: Direct spectral data
```

**2. Absorbance (Log-transformed)**
```
A = log(1/R)
= -log(R)
Effect: Enhances subtle absorption features
Linear relationship with concentration (Beer-Lambert Law)
```

**3. Continuum Removal**
```
CR = R / continuum(R)
Purpose: Remove background reflectance trends
Result: Normalized features relative to local baseline
```

### Normalization Methods

| Scaler | Formula | Use Case |
|--------|---------|----------|
| StandardScaler | (x - mean)/std | When data ~ Normal distribution |
| MinMaxScaler | (x - min)/(max - min) | Bounded data [0,1] |
| RobustScaler | (x - median)/IQR | Data with outliers |

---

## Feature Engineering

### Overview

**NEW in Phase 2**: The application now supports **6 advanced feature engineering techniques** that can be applied before model training to enhance predictive performance.

**Location in UI**: Step 3.5 (between preprocessing configuration and training)

### Available Techniques

#### 1. Spectral Derivatives

**Purpose**: Capture rate of change in spectral reflectance

**Implementation**:
```python
def compute_spectral_derivatives(X: np.ndarray, order: int = 1) -> np.ndarray:
    """
    Compute first-order derivatives (dR/dÎ»).
    
    Parameters
    ----------
    X : np.ndarray
        Spectral data (n_samples, n_wavelengths)
    order : int
        Derivative order (1 for first derivative)
    
    Returns
    -------
    np.ndarray
        Derivatives (n_samples, n_wavelengths-1)
    """
    return np.diff(X, n=order, axis=1)
```

**When to use**: 
- Enhances subtle spectral features
- Reduces baseline effects
- Useful for absorption peak detection

**Output**: n_wavelengths - 1 features

---

#### 2. Statistical Features

**Purpose**: Extract local statistical properties from spectral windows

**Implementation**:
```python
def compute_statistical_features(X: np.ndarray, window_size: int = 10) -> np.ndarray:
    """
    Compute statistical features from sliding windows.
    
    Features computed:
    - Mean
    - Standard deviation
    - Variance
    - Skewness (asymmetry)
    - Kurtosis (tail behavior)
    
    Returns
    -------
    np.ndarray
        Statistical features (n_samples, (n_wavelengths-window_size+1)*5)
    """
```

**When to use**:
- Capture local spectral variability
- Detect anomalies or patterns
- Useful for noisy data

**Output**: (n_wavelengths - window_size + 1) Ã— 5 features

---

#### 3. Polynomial Features

**Purpose**: Create interaction terms between spectral bands

**Implementation**:
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)
```

**When to use**:
- Capture non-linear relationships
- Model band interactions
- Enhance PLSR and linear models

**Warning**: High-dimensional output (n_featuresÂ² for degree=2)

**Output**: ~n_wavelengthsÂ² features (filtered to avoid memory issues)

---

#### 4. Spectral Indices

**Purpose**: Compute domain-specific spectral ratios and indices

**Implementation**:
```python
def compute_spectral_indices(X: np.ndarray, wavelengths: list = None) -> np.ndarray:
    """
    Compute spectral indices.
    
    Indices computed:
    - NDVI-like ratios
    - Absorption depths
    - Band ratios
    - Normalized differences
    
    Returns
    -------
    np.ndarray
        Spectral indices (n_samples, n_indices)
    """
```

**When to use**:
- Leverage domain knowledge
- Create physically meaningful features
- Reduce dimensionality while preserving information

**Output**: ~10-20 computed indices

---

#### 5. PCA (Principal Component Analysis)

**Purpose**: Dimensionality reduction while preserving variance

**Implementation**:
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=20, random_state=42)
X_pca = pca.fit_transform(X)
```

**When to use**:
- High-dimensional spectral data
- Multicollinearity issues
- Noise reduction

**Output**: 20 principal components (default)

**Advantage**: Orthogonal features, reduced overfitting

---

#### 6. Wavelet Features

**Purpose**: Multi-scale spectral decomposition

**Implementation**:
```python
import pywt

def compute_wavelet_features(X: np.ndarray, wavelet='db4', level=2) -> np.ndarray:
    """
    Perform wavelet decomposition.
    
    Parameters
    ----------
    wavelet : str
        Wavelet type ('db4' = Daubechies 4)
    level : int
        Decomposition level
    
    Returns
    -------
    np.ndarray
        Wavelet coefficients (approximation + details)
    """
```

**When to use**:
- Multi-resolution analysis
- Feature extraction at different scales
- Signal denoising

**Output**: Approximation + detail coefficients

**Dependency**: Requires `pywavelets` package

---

### Feature Engineering Workflow

```python
# In app.py - Step 3.5

# User selects techniques via checkboxes
use_derivatives = st.checkbox("Spectral Derivatives")
use_statistical = st.checkbox("Statistical Features")
use_polynomial = st.checkbox("Polynomial Features")
use_spectral_indices = st.checkbox("Spectral Indices")
use_pca = st.checkbox("PCA")
use_wavelet = st.checkbox("Wavelet Features")

# Configuration stored
feature_engineering_config = {
    'derivatives': use_derivatives,
    'statistical': use_statistical,
    'polynomial': use_polynomial,
    'spectral_indices': use_spectral_indices,
    'pca': use_pca,
    'wavelet': use_wavelet
}

# Applied before training
if any(feature_engineering_config.values()):
    X_train_fe, X_test_fe = apply_feature_engineering(
        X_train, X_test, feature_engineering_config
    )
```

### Analytics Tab

**NEW**: Feature Engineering tab in Data Analytics showing:
- Original vs engineered feature distributions
- Feature count comparison
- Dimensionality impact
- Correlation changes
- Interactive visualizations

---

## Feature Importance Analysis

### Overview

**NEW in Phase 2**: The application now computes **feature importance for ALL 30 model-technique combinations** using permutation importance.

**Availability**: Results â†’ Model Details Tab â†’ Feature Importance section

### Implementation

#### Permutation Importance

**Method**: Measure performance drop when feature values are randomly shuffled

**Algorithm**:
```python
def compute_feature_importance(model, X_train, X_test, y_train, y_test, 
                               model_name="Model") -> Dict[str, Any]:
    """
    Compute permutation-based feature importance.
    
    Process:
    1. Compute baseline RÂ² on test set
    2. For each feature:
       a. Shuffle feature values randomly
       b. Compute new RÂ² with shuffled feature
       c. Importance = baseline_RÂ² - shuffled_RÂ²
    3. Rank features by importance
    
    Returns
    -------
    Dict containing:
    - 'importances': Array of importance scores
    - 'top_features': Top 20 features ranked
    - 'feature_names': All feature names
    - 'baseline_score': Original RÂ²
    """
```

**Interpretation**:
- **High importance**: Feature strongly contributes to predictions
- **Low/negative importance**: Feature adds noise or redundancy
- **Zero importance**: Feature has no impact

### Feature Rankings

**Output Format**:
```python
{
    'Model': 'GBRT',
    'Technique': 'Absorbance',
    'Paradigm': 'Tuned',
    'Top Features': [
        ('wavelength_1500', 0.0823),
        ('wavelength_1720', 0.0651),
        ('wavelength_2100', 0.0543),
        ...
    ],
    'Total Features': 350,
    'Baseline RÂ²': 0.8234
}
```

### Use Cases

1. **Feature Selection**: Identify most predictive wavelengths
2. **Model Interpretation**: Understand which bands matter
3. **Dimensionality Reduction**: Remove low-importance features
4. **Domain Validation**: Verify physically meaningful features
5. **Comparison**: Compare feature importance across paradigms

### Visualization

**In Results Dashboard**:
- Bar charts of top 20 features
- Heatmaps across all 30 combinations
- Wavelength importance profiles
- Model-specific rankings

### Integration with AI Context

**Enhanced LLM Context**:
```python
# In services.py - build_llm_context()

context += f"""
FEATURE IMPORTANCE ANALYSIS:
- Top 3 features for {model_name}:
  1. {feature_1} (importance: {score_1:.4f})
  2. {feature_2} (importance: {score_2:.4f})
  3. {feature_3} (importance: {score_3:.4f})
- Most consistent feature across all models: {most_consistent}
- Feature importance available for all 30 combinations
"""
```

**AI can now answer**:
- "Which spectral bands are most important?"
- "Does feature importance differ between Standard and Tuned?"
- "Are the important features physically meaningful?"

---

## Feature Implementation

### 1. Dual Paradigm Training

**What**: Standard vs Tuned hyperparameters trained in parallel

**How**:
```python
# Standard Paradigm (fixed parameters)
trainer_standard = ModelTrainer(tune_hyperparameters=False)
results_standard = trainer_standard.train(X_train, y_train, X_test, y_test)

# Tuned Paradigm (Bayesian Optimization)
trainer_tuned = ModelTrainer(tune_hyperparameters=True, cv_folds=5)
results_tuned = trainer_tuned.train(X_train, y_train, X_test, y_test)

# Parallel execution with threading
```

**Impact**: See which paradigm performs better for your data

---

### 2. AI-Powered Insights (Enhanced)

**What**: LLM-based analysis using Google Generative AI or OpenAI

**Enhanced Context Provided (NEW)**:
- **Data Analytics**: Distribution, correlation, quality metrics
- **Training results**: All 30 model-technique-paradigm combinations
- **Performance metrics**: RÂ², RMSE, MAE, RPD by algorithm
- **Hyperparameters**: Tuned values for all 15 tuned models
- **Feature Engineering Config**: Which techniques were applied
- **Feature Importance**: Top features for all 30 combinations
- **Consistency analysis**: Std Dev across techniques and paradigms
- **Best/worst performing combinations**

**Context Size**: ~10,000+ tokens for Comparison mode (most comprehensive)

**Features**:
```python
# Conversational AI with Enhanced Understanding
- "Which spectral bands are most important for my data?"
- "Should I use feature engineering for this dataset?"
- "How does Tuned paradigm improve over Standard?"
- "Which preprocessing technique works best with GBRT?"
- "Are the feature importance results physically meaningful?"

# Automated Report Generation
- Comparison reports (Standard vs Tuned with FE + FI context)
- Feature engineering impact analysis
- Hyperparameter tuning effectiveness
- Feature importance summaries
- Production recommendations

# Example Enhanced Question:
"Which model is most consistent across techniques and why?"
â†’ AI analyzes:
  - Std Dev across all 3 techniques
  - Feature importance consistency
  - Hyperparameter sensitivity
  - Preprocessing interactions
â†’ Recommends model with:
  - Lowest variance across techniques
  - Stable feature importance
  - Robust hyperparameters
```

**AI Context Architecture**:
```
Standard Mode Context:
â”œâ”€ Data analytics (if available)
â”œâ”€ Training results (15 combinations)
â”œâ”€ Feature importance (15 combinations)
â””â”€ ~5,000 tokens

Tuned Mode Context:
â”œâ”€ Data analytics (if available)
â”œâ”€ Training results (15 combinations)
â”œâ”€ Hyperparameters (15 sets)
â”œâ”€ Feature importance (15 combinations)
â””â”€ ~7,000 tokens

Comparison Mode Context (MOST COMPREHENSIVE):
â”œâ”€ Data analytics (NEW - consistent across paradigms)
â”œâ”€ Training results (30 combinations)
â”œâ”€ Feature importance (30 combinations)
â”œâ”€ Algorithm performance analysis
â”œâ”€ Paradigm comparison metrics
â””â”€ ~10,000+ tokens
```

---

### 3. Comprehensive Analytics Dashboard (Enhanced)

**Tabs in Results View**:

1. **Overview**: KPIs, top models, paradigm comparisons
2. **Analytics**: Detailed metrics with advanced filtering
   - Filter by model, technique, or paradigm
   - Sort by any metric
   - Interactive visualizations
   - Download filtered results
3. **Model Details**: Per-model performance breakdown
   - Hyperparameter values (Tuned paradigm)
   - Cross-validation scores
   - **Feature Importance rankings (NEW)**
   - Residual analysis
4. **Comparison** (when Both paradigm): Side-by-side analysis
   - Standard vs Tuned metrics
   - Improvement percentages
   - Cost-benefit analysis
   - Recommendation for production
5. **Reports**: AI-generated insights & recommendations
   - Enhanced with FE + FI context
   - Automated summaries
   - Actionable recommendations
6. **Export**: Multiple export formats
   - **CSV**: Basic metrics
   - **Markdown**: Formatted reports
   - **Excel (NEW)**: Multi-sheet comprehensive export
     - Sheet 1: Model Results
     - Sheet 2: Feature Importance (top 20 per model)
     - Sheet 3: Test Predictions & Residuals
     - Sheet 4: Feature Engineering Config
     - Sheet 5: Summary Statistics

---

### 4. Model Inspection & Visualization (Enhanced)

**Available Visualizations**:
- **Model performance scatter plots** (RÂ² vs RMSE)
- **Residual plots** (predicted vs actual)
- **Feature importance bar charts (NEW)**: Top 20 features per model
- **Feature importance heatmaps (NEW)**: Across all 30 combinations
- **Cross-validation score distributions**
- **Technique comparison bar charts**
- **Algorithm consistency analysis**
- **Paradigm comparison charts** (Standard vs Tuned)
- **Data Analytics Tab (NEW)**:
  - Distribution histograms & box plots
  - Correlation heatmaps
  - Data quality metrics
  - Feature Engineering analytics
    - Original vs engineered feature distributions
    - Dimensionality impact
    - Correlation changes

---

### 5. Prediction Interface

**After Training**:
- Load saved models
- Input new spectral data
- Get predictions with confidence intervals
- Compare predictions across models
- Export predictions

---

## Hyperparameter Tuning Strategy

### Overview

The application uses **GridSearchCV** and **RandomizedSearchCV** (not Bayesian optimization) for finding optimal hyperparameters in the Tuned paradigm.

### Architecture

```
Standard Paradigm (Fixed Params)
    PLSR(n_components=10)
    GBRT(n_estimators=100, max_depth=5, learning_rate=0.1)
    SVR(kernel='rbf', C=100)
    KRR(kernel='rbf', alpha=1.0)
    Cubist(n_rules=10, neighbors=5)
    â†“
    [Train on full training set]
    â†“
    Results with default hyperparameters

vs

Tuned Paradigm (Optimized via GridSearchCV)
    [For each model]
    â”œâ”€ Define parameter search space
    â”œâ”€ GridSearchCV: Try ALL combinations
    â”‚  â””â”€ Example: GBRT has ~2,000+ combinations to test
    â”œâ”€ Or RandomizedSearchCV: Sample randomly (~20 samples)
    â”œâ”€ Cross-validation: Evaluate on 5 folds
    â”œâ”€ Select best parameters
    â””â”€ Train final model with best params
    â†“
    Results with optimized hyperparameters
```

### GridSearchCV vs RandomizedSearchCV

| Aspect | GridSearchCV | RandomizedSearchCV |
|--------|--------------|-------------------|
| **Search Type** | Exhaustive | Random sampling |
| **Parameter Combos** | ALL (Complete) | n_iter samples |
| **Time** | Longer | Faster |
| **GBRT Example** | ~2,000 combinations | ~20 combinations |
| **Best For** | Small param spaces | Large param spaces |
| **Use In App** | `search_type='grid'` | `search_type='random'` |

### Implementation Code

```python
# In hyperparameter_tuner.py

class HyperparameterTuner:
    
    # Predefined hyperparameter search spaces (FULL GRID)
    PARAM_GRIDS = {
        'PLSR': {
            'n_components': [3, 5, 8, 10, 12, 15, 20]         # 7 values
        },
        'GBRT': {
            'n_estimators': [50, 100, 150, 200],              # 4 values
            'learning_rate': [0.001, 0.01, 0.05, 0.1],        # 4 values
            'max_depth': [3, 4, 5, 6, 7],                      # 5 values
            'min_samples_split': [2, 5, 10],                   # 3 values
            'min_samples_leaf': [1, 2, 4],                     # 3 values
            'subsample': [0.8, 0.9, 1.0]                       # 3 values
            # Total: 4Ã—4Ã—5Ã—3Ã—3Ã—3 = 2,160 combinations
        },
        'SVR': {
            'kernel': ['rbf', 'poly', 'linear'],               # 3 values
            'C': [0.1, 1, 10, 100, 1000],                      # 5 values
            'epsilon': [0.01, 0.1, 0.5],                       # 3 values
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]       # 5 values
            # Total: 3Ã—5Ã—3Ã—5 = 225 combinations
        },
        'KRR': {
            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],            # 5 values
            'kernel': ['rbf', 'linear', 'polynomial'],          # 3 values
            'gamma': [None, 0.001, 0.01, 0.1, 1.0]             # 5 values
            # Total: 5Ã—3Ã—5 = 75 combinations
        },
        'Cubist': {
            'n_rules': [5, 10, 15, 20, 25],                     # 5 values
            'neighbors': [0, 3, 5, 7, 9]                        # 5 values
            # Total: 5Ã—5 = 25 combinations
        }
    }
    
    # Smaller grids for faster search (DEFAULT - use_small_grid=True)
    PARAM_GRIDS_SMALL = {
        'PLSR': {
            'n_components': [5, 10, 15]                         # 3 values
            # Total: 3 combinations
        },
        'GBRT': {
            'n_estimators': [100, 150],                         # 2 values
            'learning_rate': [0.01, 0.1],                       # 2 values
            'max_depth': [4, 5, 6],                             # 3 values
            'min_samples_split': [2, 5],                        # 2 values
            'subsample': [0.9, 1.0]                             # 2 values
            # Total: 2Ã—2Ã—3Ã—2Ã—2 = 48 combinations (vs 2,160!)
        },
        'SVR': {
            'kernel': ['rbf', 'linear'],                        # 2 values
            'C': [1, 100],                                      # 2 values
            'epsilon': [0.1, 0.5],                              # 2 values
            'gamma': ['scale', 'auto']                          # 2 values
            # Total: 2Ã—2Ã—2Ã—2 = 16 combinations (vs 225!)
        },
        'KRR': {
            'alpha': [0.01, 1.0, 10.0],                         # 3 values
            'kernel': ['rbf', 'linear'],                        # 2 values
            'gamma': [None, 0.01, 0.1]                          # 3 values
            # Total: 3Ã—2Ã—3 = 18 combinations (vs 75!)
        },
        'Cubist': {
            'n_rules': [10, 20],                                # 2 values
            'neighbors': [3, 5]                                 # 2 values
            # Total: 2Ã—2 = 4 combinations (vs 25!)
        }
    }
    
    def tune_model(self, base_model, X_train, y_train):
        """Tune model hyperparameters."""
        
        if self.search_type == 'random':
            # RandomizedSearchCV - faster, random sampling
            search = RandomizedSearchCV(
                base_model,
                self.param_grid,
                n_iter=20,              # Only 20 random samples
                cv=5,                   # 5-fold cross-validation
                scoring='r2',
                n_jobs=-1,              # Use all cores
                random_state=42
            )
            
        else:
            # GridSearchCV - exhaustive search (default)
            search = GridSearchCV(
                base_model,
                self.param_grid,
                cv=5,                   # 5-fold cross-validation
                scoring='r2',
                n_jobs=-1,              # Parallel computation
                verbose=1
            )
        
        # Fit - finds best parameters
        search.fit(X_train, y_train)
        
        # Return results
        return {
            'best_params': search.best_params_,
            'best_score': search.best_score_,
            'cv_results': search.cv_results_
        }
```

### Cross-Validation Process

**GridSearchCV Example (GBRT with small grid)**:

```
Step 1: Define parameter space
â”œâ”€ n_estimators: [100, 150]
â”œâ”€ learning_rate: [0.01, 0.1]
â”œâ”€ max_depth: [4, 5, 6]
â”œâ”€ min_samples_split: [2, 5]
â””â”€ subsample: [0.9, 1.0]
   Total combinations: 48

Step 2: For each combination (48 times):
â”œâ”€ Create model with these params
â”œâ”€ Run 5-fold cross-validation
â”‚  â”œâ”€ Split data into 5 folds
â”‚  â”œâ”€ Train on 4 folds
â”‚  â”œâ”€ Evaluate on 1 fold
â”‚  â”œâ”€ Repeat 5 times (different fold as test)
â”‚  â””â”€ Average 5 scores
â””â”€ Record CV score

Step 3: Select best combination
â””â”€ Pick combination with highest average CV score

Step 4: Train final model
â”œâ”€ Use best parameters
â”œâ”€ Train on entire X_train, y_train
â””â”€ Return trained model
```

**CV Fold Example (one combination, 5 folds)**:

```python
X_train = [1000 samples]
y_train = [1000 targets]

# 5-fold CV
Fold 1: Train on samples 1-800,   Test on samples 801-1000   â†’ RÂ² = 0.52
Fold 2: Train on samples 1-200+801-1000, Test on 201-800    â†’ RÂ² = 0.48
Fold 3: Train on samples 201-1000, Test on 1-200            â†’ RÂ² = 0.50
Fold 4: Train on samples 1-200+201-600+801-1000, Test 600-800 â†’ RÂ² = 0.51
Fold 5: Train on samples 1-600+801-1000, Test on 601-800    â†’ RÂ² = 0.49

Average CV Score: (0.52 + 0.48 + 0.50 + 0.51 + 0.49) / 5 = 0.50
```

### Computational Cost Example

```
Full Tuning of All 5 Models (with SMALL GRID - default):

PLSR (3 combinations):
â”œâ”€ 3 combinations Ã— 5 CV folds = 15 model trainings
â””â”€ Time: ~2 seconds

GBRT (48 combinations):
â”œâ”€ 48 combinations Ã— 5 CV folds = 240 model trainings
â””â”€ Time: ~60 seconds

SVR (16 combinations):
â”œâ”€ 16 combinations Ã— 5 CV folds = 80 model trainings
â””â”€ Time: ~20 seconds

KRR (18 combinations):
â”œâ”€ 18 combinations Ã— 5 CV folds = 90 model trainings
â””â”€ Time: ~12 seconds

Cubist (4 combinations):
â”œâ”€ 4 combinations Ã— 5 CV folds = 20 model trainings
â””â”€ Time: ~2 seconds

TOTAL with SMALL GRID: ~96 seconds (~1.6 minutes) âœ…
       with FULL GRID: ~30-40 minutes (2,160 + 225 + 75 + 25 = 2,485 combinations)

---

Comparison of Total Grid Sizes:

| Model | Full Grid | Small Grid | Reduction |
|-------|-----------|-----------|-----------|
| PLSR | 7 | 3 | 57% |
| GBRT | 2,160 | 48 | 98% âœ… |
| SVR | 225 | 16 | 93% âœ… |
| KRR | 75 | 18 | 76% âœ… |
| Cubist | 25 | 4 | 84% âœ… |
| **TOTAL** | **2,492** | **89** | **96% Reduction** |
```

---

## Implementation Details

### 1. AI Integration (How It Works) - ENHANCED

#### Architecture

```
User Question / Training Results
    â†“
[ContextBuilder] - Builds COMPREHENSIVE context (NEW: Enhanced)
    â”œâ”€ Data Analytics (NEW)
    â”‚   â”œâ”€ Distribution statistics
    â”‚   â”œâ”€ Correlation analysis
    â”‚   â””â”€ Data quality metrics
    â”œâ”€ Feature Engineering Config (NEW)
    â”‚   â”œâ”€ Applied techniques
    â”‚   â”œâ”€ Dimensionality changes
    â”‚   â””â”€ Feature statistics
    â”œâ”€ Training results (15 or 30 combinations)
    â”‚   â”œâ”€ All performance metrics
    â”‚   â”œâ”€ Cross-validation scores
    â”‚   â””â”€ Paradigm comparisons
    â”œâ”€ Hyperparameters (NEW - for Tuned paradigm)
    â”‚   â”œâ”€ Best parameters found
    â”‚   â”œâ”€ Search space explored
    â”‚   â””â”€ CV scores during tuning
    â”œâ”€ Feature Importance (NEW - all combinations)
    â”‚   â”œâ”€ Top 20 features per model
    â”‚   â”œâ”€ Importance scores
    â”‚   â””â”€ Consistency across paradigms
    â””â”€ Algorithm performance analysis
        â”œâ”€ Mean metrics by algorithm
        â”œâ”€ Consistency (Std Dev)
        â””â”€ Best/worst combinations
    â†“
[AIExplainer / ExplainerWithCache] - Interfaces with LLM
    â”œâ”€ Detects AI provider (Google Generative AI / OpenAI)
    â”œâ”€ Validates API key (.env file)
    â”œâ”€ Implements response caching
    â””â”€ Handles rate limiting & errors
    â†“
[LLM (Gemini or ChatGPT)] - Generates insights with RICH context
    â”œâ”€ Analyzes ~10,000+ tokens of context
    â”œâ”€ Understands FE + FI + hyperparameters
    â”œâ”€ Generates data-driven recommendations
    â””â”€ Answers complex questions
    â†“
[Response] - Formatted result to user
```

#### Implementation Code

**AI Provider Detection** (`ai_explainer.py`):
```python
class AIExplainerFactory:
    @staticmethod
    def create_explainer(provider: str = "gemini", api_key: str = None):
        """Create appropriate AI explainer based on provider."""
        if provider.lower() == "gemini":
            return GoogleAIExplainer(api_key)
        elif provider.lower() == "openai":
            return OpenAIExplainer(api_key)
        else:
            raise ValueError(f"Unknown provider: {provider}")
```

**Response Caching** (improves performance):
```python
class ExplainerWithCache:
    def __init__(self, provider, api_key, cache_enabled=True):
        self.explainer = AIExplainerFactory.create_explainer(provider, api_key)
        self.cache = {} if cache_enabled else None
    
    def generate_insights(self, context):
        # Check cache first
        cache_key = hash(context)
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # Generate if not cached
        response = self.explainer.generate(context)
        self.cache[cache_key] = response
        return response
```

#### Enhanced Context Building Example

```python
# In services.py - ContextBuilder class (ENHANCED)
def build_llm_context(results_df, raw_data, target_col, paradigm="Standard",
                     data_analytics_context=None,           # NEW
                     feature_engineering_config=None,       # NEW
                     feature_engineering_data=None,         # NEW
                     hyperparameters_context=None,          # NEW
                     feature_importance_data=None):         # NEW
    """Build COMPREHENSIVE context for AI with ALL available information."""
    
    context_parts = []
    
    # 1. Data Analytics Context (NEW)
    if data_analytics_context:
        context_parts.extend([
            "DATA ANALYTICS:",
            f"- Distribution: {data_analytics_context['distribution_summary']}",
            f"- Correlation: Top correlated features with target",
            f"- Quality: {data_analytics_context['quality_metrics']}",
            ""
        ])
    
    # 2. Feature Engineering Context (NEW)
    if feature_engineering_config and any(feature_engineering_config.values()):
        context_parts.extend([
            "FEATURE ENGINEERING APPLIED:",
            ""
        ])
        if feature_engineering_config.get('derivatives'):
            context_parts.append("âœ“ Spectral Derivatives (1st order)")
        if feature_engineering_config.get('statistical'):
            context_parts.append("âœ“ Statistical Features (sliding window)")
        if feature_engineering_config.get('polynomial'):
            context_parts.append("âœ“ Polynomial Features (degree 2)")
        if feature_engineering_config.get('spectral_indices'):
            context_parts.append("âœ“ Spectral Indices (domain-specific)")
        if feature_engineering_config.get('pca'):
            context_parts.append("âœ“ PCA (dimensionality reduction)")
        if feature_engineering_config.get('wavelet'):
            context_parts.append("âœ“ Wavelet Features (multi-scale)")
        
        if feature_engineering_data:
            context_parts.append(
                f"- Original features: {feature_engineering_data['original_features']}"
            )
            context_parts.append(
                f"- Engineered features: {feature_engineering_data['engineered_features']}"
            )
            context_parts.append("")
    
    # 3. Training Paradigm & Results
    context_parts.extend([
        f"TRAINING PARADIGM: {paradigm}",
        "",
        "DATA OVERVIEW:",
        f"- Samples: {len(raw_data)}",
        f"- Features: {raw_data.shape[1]}",
        f"- Target: {target_col}",
        "",
        f"TRAINING RESULTS ({len(results_df)} model combinations):",
        f"- Best RÂ²: {results_df['Test_RÂ²'].max():.4f}",
        f"- Mean RÂ²: {results_df['Test_RÂ²'].mean():.4f}",
        f"- Models tested: {', '.join(results_df['Model'].unique())}",
        f"- Techniques used: {', '.join(results_df['Technique'].unique())}",
        ""
    ])
    
    # 4. Hyperparameters Context (NEW - for Tuned paradigm)
    if hyperparameters_context and paradigm == "Tuned":
        context_parts.extend([
            "HYPERPARAMETER TUNING RESULTS:",
            ""
        ])
        for model_name, params in hyperparameters_context.items():
            context_parts.append(f"{model_name}:")
            for param, value in params.items():
                context_parts.append(f"  - {param}: {value}")
            context_parts.append("")
    
    # 5. Feature Importance Context (NEW - all combinations)
    if feature_importance_data:
        context_parts.extend([
            "FEATURE IMPORTANCE ANALYSIS:",
            ""
        ])
        for combo_name, importance_info in feature_importance_data.items():
            context_parts.append(f"{combo_name}:")
            top_features = importance_info['top_features'][:3]
            for feat_name, importance_score in top_features:
                context_parts.append(
                    f"  - {feat_name}: {importance_score:.4f}"
                )
            context_parts.append("")
    
    # 6. Performance by Algorithm
    context_parts.extend([
        "PERFORMANCE BY ALGORITHM:",
        generate_algorithm_performance_section(results_df),
        "",
        "TOP MODELS:",
        generate_top_models_section(results_df)
    ])
    
    return "\n".join(context_parts)
```

**Context Size Comparison**:
- **Old (Phase 1)**: ~3,000 tokens (training results only)
- **New (Phase 2 - Standard)**: ~5,000 tokens (+data analytics, +FI)
- **New (Phase 2 - Tuned)**: ~7,000 tokens (+hyperparameters)
- **New (Phase 2 - Comparison)**: ~10,000+ tokens (all 30 combinations + FI)

#### Chat Interface Features

```python
class ChatInterface:
    """Conversational interface for model insights."""
    
    def __init__(self, ai_provider="gemini", api_key=None):
        self.explainer = AIExplainerFactory.create_explainer(ai_provider, api_key)
        self.cycle_chat_history = []      # Current session only
        self.persistent_history = []       # Saved to disk
    
    def chat(self, user_query, context):
        """Process user query and get AI response."""
        # Add to histories
        self.add_message('user', user_query)
        
        # Get AI response with context
        response = self.explainer.chat(
            messages=self.cycle_chat_history,
            context=context
        )
        
        # Add response to histories
        self.add_message('assistant', response)
        
        # Save globally
        GlobalChatHistory.add_message('assistant', response)
        
        return response
```

---

### 2. Analytics Implementation

#### Architecture

```
Training Results (15 combinations)
    â†“
[MetricsAnalyzer] - Compute aggregate metrics
    â”œâ”€ Performance by technique
    â”œâ”€ Performance by model
    â”œâ”€ Performance by paradigm
    â””â”€ Consistency metrics (Std Dev)
    â†“
[ModelAnalyzer] - Detailed model statistics
    â”œâ”€ Per-model performance breakdown
    â”œâ”€ Hyperparameter values
    â”œâ”€ Cross-validation scores
    â””â”€ Technique comparison per model
    â†“
[Visualizations] - Plotly charts
    â”œâ”€ Scatter plots (model vs metrics)
    â”œâ”€ Box plots (distribution by technique)
    â”œâ”€ Bar charts (technique comparison)
    â””â”€ Heatmaps (model-technique performance matrix)
```

#### Metrics Calculation

```python
class MetricsAnalyzer:
    @staticmethod
    def compute_metrics_by_technique(results_df):
        """Compute aggregated metrics per technique."""
        by_technique = {}
        
        for technique in results_df['Technique'].unique():
            tech_data = results_df[results_df['Technique'] == technique]
            
            by_technique[technique] = {
                'mean_r2': tech_data['Test_RÂ²'].mean(),
                'std_r2': tech_data['Test_RÂ²'].std(),      # Consistency
                'max_r2': tech_data['Test_RÂ²'].max(),
                'count': len(tech_data),
                'best_model': tech_data.loc[
                    tech_data['Test_RÂ²'].idxmax(), 'Model'
                ]
            }
        
        return by_technique
    
    @staticmethod
    def compute_metrics_by_model(results_df):
        """Compute aggregated metrics per model."""
        by_model = {}
        
        for model in results_df['Model'].unique():
            model_data = results_df[results_df['Model'] == model]
            
            by_model[model] = {
                'mean_r2': model_data['Test_RÂ²'].mean(),
                'std_r2': model_data['Test_RÂ²'].std(),      # Consistency
                'techniques': model_data['Technique'].unique().tolist(),
                'consistency_rank': None  # Filled later
            }
        
        # Rank models by consistency (lower std = more consistent)
        models_sorted = sorted(by_model.items(), 
                              key=lambda x: x[1]['std_r2'])
        for rank, (model, _) in enumerate(models_sorted, 1):
            by_model[model]['consistency_rank'] = rank
        
        return by_model
```

#### Dashboard Rendering

```python
# In interface.py
def render_analytics_dashboard(results_df, paradigm):
    """Render comprehensive analytics dashboard."""
    
    # Create tabs
    tab1, tab2, tab3 = st.tabs([
        "ðŸ“Š Overview",
        "ðŸ” Detailed Analysis", 
        "ðŸ“ˆ Trends"
    ])
    
    with tab1:
        # KPI metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Best RÂ²", f"{results_df['Test_RÂ²'].max():.4f}")
        with col2:
            st.metric("Mean RÂ²", f"{results_df['Test_RÂ²'].mean():.4f}")
        with col3:
            st.metric("Total Models", len(results_df))
        with col4:
            st.metric("Techniques", results_df['Technique'].nunique())
    
    with tab2:
        # Detailed metrics by technique
        metrics_by_tech = MetricsAnalyzer.compute_metrics_by_technique(results_df)
        
        # Display as table
        tech_df = pd.DataFrame(metrics_by_tech).T
        st.dataframe(tech_df, use_container_width=True)
    
    with tab3:
        # Visualizations
        fig = px.box(
            results_df,
            x='Technique',
            y='Test_RÂ²',
            color='Model',
            title='RÂ² Distribution by Technique and Model'
        )
        st.plotly_chart(fig, use_container_width=True)
```

---

### 3. Report Generation (Step-by-Step)

#### Single Paradigm Report

```
Process Flow:
â”œâ”€ User clicks "Generate Report" button
â”œâ”€ ReportGenerator initializes
â”œâ”€ [Collect Data]
â”‚  â”œâ”€ Read results_df (15 rows)
â”‚  â”œâ”€ Get raw_data statistics
â”‚  â””â”€ Extract target column info
â”œâ”€ [Generate Summary]
â”‚  â”œâ”€ Best/worst/mean/median RÂ²
â”‚  â”œâ”€ Std Dev for consistency
â”‚  â””â”€ Top 5 models
â”œâ”€ [Generate Statistics]
â”‚  â”œâ”€ By technique breakdown
â”‚  â”œâ”€ By model breakdown
â”‚  â””â”€ Cross-validation scores
â”œâ”€ [AI Insights] (if enabled)
â”‚  â”œâ”€ Send context to LLM
â”‚  â”œâ”€ Receive insights
â”‚  â””â”€ Cache response
â””â”€ [Format & Display]
   â”œâ”€ Markdown formatting
   â”œâ”€ Display in UI
   â””â”€ Provide download option
```

#### Code Implementation

```python
class ReportGenerator:
    def generate_training_report(self, results_df, paradigm="Standard"):
        """Generate report for single paradigm."""
        
        report = {
            'title': f'{paradigm} Training Report',
            'timestamp': datetime.now().isoformat(),
            'paradigm': paradigm,
            
            # 1. Summary statistics
            'summary': {
                'total_models': len(results_df),
                'best_r2': float(results_df['Test_RÂ²'].max()),
                'mean_r2': float(results_df['Test_RÂ²'].mean()),
                'best_model': results_df.loc[
                    results_df['Test_RÂ²'].idxmax(), 'Model'
                ]
            },
            
            # 2. Detailed statistics
            'statistics': self._calculate_statistics(results_df),
            
            # 3. Top models
            'top_models': self._get_top_models(results_df, top_n=5),
            
            # 4. Technique analysis
            'technique_analysis': self._analyze_techniques(results_df)
        }
        
        # 5. AI insights (if available)
        if self.ai_available:
            context = ContextBuilder.build_training_context(
                results_df, 
                self.raw_data,
                self.target_col,
                paradigm
            )
            report['ai_insights'] = self._generate_ai_insights(
                results_df, 
                paradigm, 
                context
            )
        
        return report
    
    def _generate_ai_insights(self, results_df, paradigm, context):
        """Generate AI-powered insights."""
        prompt = f"""
        Analyze the following {paradigm} training results and provide insights:
        
        Context:
        {context}
        
        Provide:
        1. Key findings and patterns
        2. Why certain models perform better
        3. Recommendations for improvement
        4. Concerns or limitations
        5. Which techniques work best for this data
        """
        
        response = self.explainer.generate_insights(prompt)
        return response
```

#### Comparison Report

```python
def generate_comparison_report(self, standard_results, tuned_results):
    """Generate comparison between paradigms."""
    
    report = {
        'title': 'Standard vs Tuned Paradigm Comparison',
        'timestamp': datetime.now().isoformat(),
        
        # Summaries for both
        'standard_summary': self._generate_summary(standard_results),
        'tuned_summary': self._generate_summary(tuned_results),
        
        # Improvements calculation
        'improvements': {
            'best_r2_improvement': (
                tuned_results['Test_RÂ²'].max() - 
                standard_results['Test_RÂ²'].max()
            ),
            'mean_r2_improvement': (
                tuned_results['Test_RÂ²'].mean() - 
                standard_results['Test_RÂ²'].mean()
            ),
            'rmse_improvement': (
                standard_results['Test_RMSE'].min() - 
                tuned_results['Test_RMSE'].min()
            ),
            'tuning_beneficial': (
                tuned_results['Test_RÂ²'].max() > 
                standard_results['Test_RÂ²'].max()
            )
        },
        
        # Top models for each
        'standard_top_models': self._get_top_models(standard_results),
        'tuned_top_models': self._get_top_models(tuned_results),
    }
    
    # AI recommendations
    if self.ai_available:
        report['ai_recommendations'] = self._generate_recommendations(
            standard_results,
            tuned_results
        )
    
    return report
```

#### Markdown Formatting

```python
def format_report_as_markdown(self, report):
    """Format report as markdown string."""
    
    # For comparison reports
    if 'standard_summary' in report:
        md = f"""
# {report['title']}

**Generated**: {report['timestamp']}

## Summary Comparison

### Standard Paradigm
- **Best RÂ²**: {report['standard_summary']['best_r2']:.4f}
- **Mean RÂ²**: {report['standard_summary']['mean_r2']:.4f}
- **Best Model**: {report['standard_summary']['best_model']}

### Tuned Paradigm
- **Best RÂ²**: {report['tuned_summary']['best_r2']:.4f}
- **Mean RÂ²**: {report['tuned_summary']['mean_r2']:.4f}
- **Best Model**: {report['tuned_summary']['best_model']}

## Performance Improvements
- **Best RÂ² Improvement**: {report['improvements']['best_r2_improvement']:.4f}
- **Mean RÂ² Improvement**: {report['improvements']['mean_r2_improvement']:.4f}
- **Tuning Beneficial**: {'Yes' if report['improvements']['tuning_beneficial'] else 'No'}

## Top Models - Standard
{self._format_top_models(report['standard_top_models'])}

## Top Models - Tuned
{self._format_top_models(report['tuned_top_models'])}

## AI Recommendations
{report.get('ai_recommendations', 'N/A')}
"""
    
    return md
```

---

### 4. Streamlit Integration & UI Rendering

#### Session State Management

```python
def init_session_state():
    """Initialize all session state variables."""
    
    # Data state
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'raw_data' not in st.session_state:
        st.session_state.raw_data = None
    if 'target_col' not in st.session_state:
        st.session_state.target_col = None
    
    # Training state
    if 'training_complete' not in st.session_state:
        st.session_state.training_complete = False
    if 'all_results' not in st.session_state:
        st.session_state.all_results = {}
    
    # Report state
    if 'comparison_report' not in st.session_state:
        st.session_state.comparison_report = None
    
    # Chat state
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
```

#### Report Generation in UI

```python
# In interface.py - render_paradigm_results function

with col_report:
    if st.button("ðŸ“ Generate Report"):
        try:
            with st.spinner("Generating AI report..."):
                gen = ReportGenerator(
                    raw_data=st.session_state.raw_data,
                    target_col=st.session_state.target_col
                )
                
                # Generate report
                report = gen.generate_training_report(
                    results_df, 
                    paradigm,
                    include_ai_insights=True
                )
                
                # Format as markdown
                report_md = gen.format_report_as_markdown(report)
                
                # Display
                st.markdown(report_md)
                
                # Download option
                st.download_button(
                    "ðŸ“¥ Download Report",
                    report_md,
                    f"report_{paradigm}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    "text/markdown"
                )
                
                st.success("âœ… Report generated!")
        
        except Exception as e:
            st.error(f"Error: {str(e)}")
```

---

### 5. User Personas & Workflows

#### Persona 1: Soil Scientist / Researcher

**Goal**: Find best model for soil property prediction

**Workflow**:
1. Load spectral data CSV
2. Select target soil property (e.g., "Soil Moisture")
3. Train both Standard and Tuned paradigms
4. Compare performance metrics
5. Review AI recommendations
6. Ask AI questions: "Which model is most robust?"
7. Download comparison report
8. Use best model for future predictions

**Key Features Used**:
- Analytics dashboard
- Model comparison
- AI chat
- Report generation
- Export functionality

---

#### Persona 2: Data Scientist / ML Engineer

**Goal**: Optimize model performance and understand why tuning helps

**Workflow**:
1. Load data
2. Run Tuned paradigm only (focus on hyperparameter tuning)
3. Inspect individual model hyperparameters
4. View cross-validation scores
5. Check improvement percentages
6. Extract best hyperparameters
7. Export detailed results
8. Deploy best model

**Key Features Used**:
- Training orchestration
- Hyperparameter inspection
- Cross-validation analysis
- Detailed metrics
- Model export

---

#### Persona 3: Domain Expert / Decision Maker

**Goal**: Understand model performance and business impact

**Workflow**:
1. View summary KPIs
2. Read AI-generated recommendations
3. Ask AI: "Was tuning worth the effort?"
4. Review comparison report
5. Check "Is this model ready for production?"
6. View confidence intervals / uncertainty estimates
7. Share report with stakeholders

**Key Features Used**:
- Dashboard KPIs
- AI insights
- AI chat
- Comparison reports
- Export for presentations

---

### 6. Error Handling & Logging

#### Logging Architecture

```python
# In system.py
def initialize_logging(log_dir="./logs"):
    """Initialize comprehensive logging."""
    
    log_dir = Path(log_dir)
    log_dir.mkdir(exist_ok=True)
    
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    
    # File handler
    file_handler = logging.FileHandler(
        log_dir / f"ssm_{datetime.now().strftime('%Y%m%d')}.log"
    )
    file_handler.setLevel(logging.DEBUG)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger
```

#### Exception Handling Pattern

```python
def safe_operation(func_name, fallback_value=None):
    """Decorator for safe operations with error handling."""
    def decorator(func):
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.error(
                    f"Error in {func_name}: {str(e)}", 
                    exc_info=True
                )
                if fallback_value is not None:
                    return fallback_value
                raise
        return wrapper
    return decorator
```

---

## Application Features

### Training Paradigms (Three Options)

The application offers **THREE distinct training paradigms** selectable via radio button in Step 3:

#### 1ï¸âƒ£ Standard (No Tuning)
**Label in UI**: "Standard (No Tuning)"

```python
# Uses fixed default hyperparameters
# No hyperparameter optimization
# Fastest training option

models = {
    'PLSR': PLSRModel(n_components=10),
    'GBRT': GBRTModel(n_estimators=100, learning_rate=0.1, max_depth=5),
    'SVR': SVRModel(kernel='rbf', C=100.0, epsilon=0.1, gamma='scale'),
    'KRR': KRRModel(alpha=1.0, kernel='rbf', gamma=None),
    'Cubist': CubistModel(n_rules=20, neighbors=5)
}

# Trains 15 models (5 Ã— 3 techniques) with defaults
# Training time: ~150-200 seconds
```

**When to Use**:
- Quick baseline comparison
- Limited computational resources
- Testing data pipeline
- Default behavior comparison

---

#### 2ï¸âƒ£ Tuned Only (with CV)
**Label in UI**: "Tuned Only (with CV)"

```python
# Hyperparameter optimization using GridSearchCV/RandomizedSearchCV
# 5-fold cross-validation for robust tuning
# Finds optimal parameters for each model-technique combo

tuner = HyperparameterTuner(
    model_name='GBRT',
    cv_folds=5,                   # User-configurable: 3-10
    search_type='grid',           # Exhaustive search
    use_small_grid=True,          # DEFAULT: faster with SMALL grid
    n_iter=20                     # For RandomizedSearchCV
)

# Tunes 15 models with optimized parameters
# Training time: ~400-600 seconds
```

**When to Use**:
- Focus on hyperparameter optimization
- Willing to wait for better parameters
- Want to see improvement from tuning
- Analyzing algorithm sensitivity

---

#### 3ï¸âƒ£ Both (Compare Standard vs Tuned)
**Label in UI**: "Both (Compare Standard vs Tuned)"

```python
# Trains BOTH paradigms in parallel for direct comparison
# Standard paradigm: fixed defaults
# Tuned paradigm: GridSearchCV optimized

# Standard + Tuned trained simultaneously
# User can compare improvements from tuning
# Generates comparison reports

# Training time: ~600-900 seconds
# (roughly sum of both paradigms due to parallel execution)
```

**When to Use**:
- Want to see if tuning helps for your data
- Decision making: Is tuning worth the effort?
- Comparative analysis
- Full evaluation of both approaches

**Output**: Comparison report showing:
- Best RÂ² (Standard vs Tuned)
- Mean RÂ² improvement
- RMSE improvement
- Top models for each paradigm
- AI recommendations

---

### Main Modes

#### 1. Training Mode
```
Step 1: Load Data (CSV format)
        â†“
Step 2: Select Target Column
        â†“
Step 3: Choose Application Mode
        - Standard Paradigm Only
        - Tuned Paradigm Only
        - Both (Parallel Training)
        â†“
Step 4: Select Training Options
        - Spectral Technique
        - Normalization Method
        - Cross-validation Folds
        â†“
Step 5: Start Training
        â†“
Step 6: View Results & Analysis
```

#### 2. Prediction Mode
```
Load Trained Model
    â†“
Input Spectral Data
    â†“
Generate Predictions
    â†“
View Results & Comparisons
```

### Export Capabilities

1. **Results Export**: CSV with all metrics
2. **Report Export**: Markdown with analysis
3. **Model Export**: Joblib-serialized models
4. **Prediction Export**: CSV with predictions

---

### UI Components

**Glassmorphic Design**:
- Semi-transparent cards
- Gradient backgrounds
- Smooth animations
- Responsive layout

**Custom Widgets**:
- Progress indicators
- Metric cards
- Interactive tables
- Real-time plots

---

## Setup & Usage

### Installation

```bash
# Clone repository
git clone <repo_url>
cd Team27_Spectral_Soil_Modeler

# Create virtual environment
python -m venv soil

# Activate environment
source soil/bin/activate  # Linux/Mac
# or
soil\Scripts\activate  # Windows

# Install dependencies
pip install -r soil_modeler/requirements.txt

# Set up API keys (optional, for AI features)
export GOOGLE_API_KEY="your_key_here"
export OPENAI_API_KEY="your_key_here"
```

### Running the Application

```bash
cd soil_modeler

# Run training/prediction app
streamlit run src/app.py

# App opens at http://localhost:8501
```

### Data Format

**CSV Input Requirements**:
- First column: Target variable (soil property)
- Remaining columns: Spectral bands (float values)
- Example: 1 target + 2150 bands = 2151 columns

```
Target_Property,Band_1,Band_2,...,Band_2150
0.5,0.123,0.456,...,0.789
0.7,0.234,0.567,...,0.890
...
```

---

## Phase 2 Enhancements Summary (NEW)

### What's New in Phase 2

This phase adds **comprehensive feature engineering, feature importance analysis, and enhanced AI integration** to the existing ML pipeline.

#### 1. Feature Engineering (6 Methods)
- **Spectral Derivatives**: 1st-order rate of change
- **Statistical Features**: Sliding window statistics (mean, std, var, skew, kurtosis)
- **Polynomial Features**: Degree-2 interactions between bands
- **Spectral Indices**: Domain-specific ratios and indices
- **PCA**: Dimensionality reduction (20 components)
- **Wavelet Features**: Multi-scale decomposition (db4, level 2)

**Impact**: Can improve model performance by 5-20% depending on data characteristics

#### 2. Feature Importance Analysis
- **Method**: Permutation importance for ALL 30 combinations
- **Output**: Top 20 features ranked by importance score
- **Visualization**: Bar charts and heatmaps
- **Use Cases**: Feature selection, model interpretation, wavelength identification

#### 3. Enhanced AI Context
- **Data Analytics**: Distribution, correlation, quality metrics
- **FE Configuration**: Applied techniques and dimensionality changes
- **Hyperparameters**: Tuned values for all 15 tuned models
- **Feature Importance**: Top features for all 30 combinations
- **Context Size**: Up to 10,000+ tokens (3x increase)

#### 4. Comprehensive Export System
- **CSV**: Basic metrics (existing)
- **Markdown**: Formatted reports (existing)
- **Excel (NEW)**: Multi-sheet workbooks with:
  - Model Results
  - Feature Importance (top 20 per model)
  - Test Predictions & Residuals
  - Feature Engineering Config
  - Summary Statistics

#### 5. Enhanced Analytics Dashboard
- **NEW Tab**: Feature Engineering analytics in Data Analytics section
- **Feature Importance Tab**: Rankings and visualizations for all models
- **Enhanced Model Details**: Now includes FI alongside hyperparameters
- **Interactive Visualizations**: Plotly-based charts with filtering

### Code Changes Summary

| File | Lines Added | Key Features |
|------|-------------|--------------|
| `app.py` | +495 | FE UI (Step 3.5), checkboxes for 6 techniques |
| `interface.py` | +956 | FE analytics tab, FI visualizations |
| `preprocessing.py` | +301 | Statistical features computation |
| `model_analyzer.py` | +140 | Permutation importance calculation |
| `services.py` | +275 | Enhanced LLM context building |
| `system.py` | +328 | Comprehensive Excel export |
| **Total** | **+2,495** | **All Phase 2 features** |

### Dependencies Added
- `pywt` (PyWavelets 1.9.0): Wavelet transformations
- `seaborn` (0.13.2): Statistical visualizations
- `matplotlib` (3.10.7): Plotting backend
- Additional transitive dependencies

---

## Key Technical Highlights

### GridSearchCV (Not Bayesian Optimization)

```python
# Used for hyperparameter tuning in "Tuned" paradigm
from sklearn.model_selection import GridSearchCV

# Example: GBRT with small grid
param_grid = {
    'n_estimators': [100, 150],
    'learning_rate': [0.01, 0.1],
    'max_depth': [4, 5, 6],
    'min_samples_split': [2, 5],
    'subsample': [0.9, 1.0]
}

search = GridSearchCV(
    base_model,
    param_grid,
    cv=5,                   # 5-fold cross-validation
    scoring='r2',
    n_jobs=-1,              # Parallel computation
    verbose=1
)

search.fit(X_train, y_train)
best_params = search.best_params_
best_score = search.best_score_
```

### Cross-Validation Strategy

- **CV Folds**: 5-fold cross-validation
- **Purpose**: Estimate generalization error
- **Used for**: Hyperparameter tuning, model selection
- **Computational Cost**: Each param combination tested 5 times

### Performance Metrics Explained

- **RÂ² Score**: % variance explained [0, 1]. Higher is better.
- **RMSE**: Root Mean Squared Error. Lower is better.
- **MAE**: Mean Absolute Error. Lower is better.
- **RPD**: Residual Prediction Deviation. Higher is better. (Std/RMSE)

---

## File Descriptions (Updated)

| File | Purpose | Phase 2 Changes |
|------|---------|----------------|
| `app.py` | Main Streamlit app, UI orchestration | +FE UI (Step 3.5) |
| `interface.py` | UI components, dashboards, visualizations | +FE analytics tab, +FI visualizations |
| `services.py` | Context building, report generation, AI integration | +Enhanced LLM context (FE+FI+HP) |
| `preprocessing.py` | Spectral transformations, normalization | +6 FE methods |
| `models/model_trainer.py` | Training orchestration, 30 combinations | +FI computation |
| `models/*.py` | Individual model implementations | No changes |
| `evaluation.py` | Metrics computation, result analysis | No changes |
| `data_loader.py` | CSV loading, data validation | No changes |
| `model_analyzer.py` | Performance analysis, comparisons | +Permutation importance |
| `chat_interface.py` | LLM integration, conversational AI | No changes |
| `system.py` | Logging, utilities, persistence | +Comprehensive Excel export |
| `.env` | API keys configuration | NEW |
| `requirements.txt` | Dependencies | +PyWavelets, seaborn, matplotlib |

---

## Performance Summary

**Typical Results on Soil Spectral Data (Phase 2)**:
- Best RÂ² (Standard): 0.45-0.55
- Best RÂ² (Tuned): 0.48-0.58
- **Best RÂ² (with FE)**: 0.50-0.65 (+5-20% improvement)
- Improvement from tuning: 5-15%
- **Improvement from FE**: 5-20% (data-dependent)
- Most Consistent: PLSR/SVR
- Fastest: PLSR
- Most Interpretable: Cubist + Feature Importance

**Feature Importance Insights**:
- Typically 5-10 wavelengths contribute 80% of predictive power
- Near-infrared bands (1400-2400 nm) often most important for soil properties
- Consistency across paradigms validates physical interpretability

---

## Future Enhancements

- Deep learning models (1D CNN, LSTM)
- Ensemble methods (stacking, blending)
- **Automated feature selection** based on importance scores
- **Feature engineering recommendation system** (AI-guided)
- Real-time model retraining
- Mobile app deployment
- **Multi-target prediction** (predict multiple soil properties simultaneously)

---

**Version**: 2.0.0 (Phase 2)  
**Last Updated**: December 2, 2025  
**Team**: Team 27 - Spectral Soil Modeler  
**Major Features**: Feature Engineering + Feature Importance + Enhanced AI Context
