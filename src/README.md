# üåæ Spectral Soil Modeler - Comprehensive Documentation

## Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [ML Pipeline](#ml-pipeline)
4. [Models & Algorithms](#models--algorithms)
5. [Data Preprocessing](#data-preprocessing)
6. [Feature Implementation](#feature-implementation)
7. [Implementation Details](#implementation-details)
8. [Application Features](#application-features)
9. [Setup & Usage](#setup--usage)

---

## Project Overview

**Spectral Soil Modeler** is an advanced machine learning application for predicting soil properties using spectral data. It implements a complete ML pipeline with:

- **15 Model-Technique Combinations**: 5 algorithms √ó 3 preprocessing techniques
- **Dual Training Paradigms**: Standard vs Tuned hyperparameters
- **Interactive Web UI**: Real-time training, prediction, and analysis
- **AI-Powered Insights**: LLM integration for recommendations
- **Comprehensive Analytics**: Metrics, comparisons, and visualizations

### Key Statistics
- **5 ML Algorithms**: PLSR, GBRT, KRR, SVR, Cubist
- **3 Spectral Techniques**: Reflectance, Absorbance, Continuum Removal
- **Dual Paradigm Training**: Standard + Tuned (parallel)
- **Cross-validation**: 5-fold for robust evaluation

---

## Architecture

```
Project Structure
‚îú‚îÄ‚îÄ soil_modeler/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # Main Streamlit application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interface.py              # UI components & rendering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services.py               # Context builder & report generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py          # Spectral data preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py            # Data loading & validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py             # Model evaluation metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_analyzer.py         # Performance analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_interface.py         # AI chat integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system.py                 # Logging & utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ model_trainer.py      # Training orchestration
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ plsr.py               # PLSR implementation
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gbrt.py               # GBRT implementation
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ krr.py                # KRR implementation
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ svr.py                # SVR implementation
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ cubist.py             # Cubist implementation
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ hyperparameter_tuner.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### Application Flow Diagram

```
User Input (CSV Data)
    ‚Üì
[Data Loading & Validation]
    ‚Üì
[Preprocessing Pipeline]
‚îú‚îÄ Reflectance Transformation
‚îú‚îÄ Absorbance Transformation (Log-based)
‚îî‚îÄ Continuum Removal
    ‚Üì
[Normalization: StandardScaler / MinMaxScaler / RobustScaler]
    ‚Üì
[Training - Two Paradigms in Parallel]
‚îú‚îÄ Standard Pipeline (fixed hyperparameters)
‚îî‚îÄ Tuned Pipeline (optimized hyperparameters)
    ‚Üì
[Model Evaluation & Cross-validation]
    ‚Üì
[Results Display & Analysis]
‚îú‚îÄ Comparison reports
‚îú‚îÄ AI-powered recommendations
‚îú‚îÄ Export capabilities
‚îî‚îÄ Prediction interface
```

---

## ML Pipeline

### Phase 1: Data Preparation

```python
# Data Loading
data = DataLoader.load_from_csv("spectral_data.csv")
# Shape: (n_samples, n_features) where features are spectral bands

# Train-Test Split: 80-20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

### Phase 2: Preprocessing

The preprocessing pipeline applies:

1. **Spectral Transformations**:
   - **Reflectance**: `R = original_data` (baseline)
   - **Absorbance**: `A = log(1/R)` (emphasizes subtle features)
   - **Continuum Removal**: `CR = R / continuum(R)` (removes background)

2. **Normalization** (3 options):
   ```python
   StandardScaler()    # (X - mean) / std
   MinMaxScaler()      # (X - min) / (max - min)
   RobustScaler()      # (X - median) / IQR (robust to outliers)
   ```

3. **Optional Feature Engineering**:
   - Derivatives (first & second order)
   - Moving averages
   - Ratio calculations

### Phase 3: Training

**Standard Paradigm**: Fixed hyperparameters (defaults)
```python
models = {
    'PLSR': PLSRModel(n_components=10),
    'GBRT': GBRTModel(n_estimators=100, learning_rate=0.1, max_depth=5),
    'SVR': SVRModel(kernel='rbf', C=100.0, epsilon=0.1, gamma='scale'),
    'KRR': KRRModel(alpha=1.0, kernel='rbf', gamma=None),
    'Cubist': CubistModel(n_rules=20, neighbors=5)
}

# Results: 15 models (5 models √ó 3 techniques) with default parameters
```

**Tuned Paradigm**: GridSearchCV / RandomizedSearchCV for optimization
```python
# Uses HyperparameterTuner with two approaches:
# 1. GridSearchCV: Exhaustive search (default)
# 2. RandomizedSearchCV: Random sampling (faster)

tuner = HyperparameterTuner(
    model_name='GBRT',
    cv_strategy='k-fold',         # 'k-fold' or 'leave-one-out'
    cv_folds=5,                   # K-fold splits (for k-fold strategy)
    search_type='grid',           # 'grid' (exhaustive) or 'random' (sampling)
    n_iter=20,                    # Random samples (for RandomizedSearchCV only)
    use_small_grid=True           # Use smaller grid for faster search (DEFAULT)
)

best_params = tuner.tune_model(base_model, X_train, y_train)
# Returns: best_params, best_score, cv_results

# Results: 15 models (5 models √ó 3 techniques) with OPTIMIZED parameters
```

### Phase 4: Evaluation

For each model:
- **Train Metrics**: R¬≤, RMSE, MAE on training set
- **Test Metrics**: R¬≤, RMSE, MAE on test set
- **Cross-validation**: 5-fold CV scores
- **RPD**: Residual Prediction Deviation

```python
metrics = {
    'Train_R¬≤': r2_score(y_train, y_pred_train),
    'Test_R¬≤': r2_score(y_test, y_pred_test),
    'Train_RMSE': sqrt(mse(y_train, y_pred_train)),
    'Test_RMSE': sqrt(mse(y_test, y_pred_test)),
    'Test_MAE': mean_absolute_error(y_test, y_pred_test),
    'RPD': std(y_test) / rmse(y_test, y_pred_test)
}
```

---

## Models & Algorithms

### 1. PLSR (Partial Least Squares Regression)

**Purpose**: Handle multicollinear spectral data by finding latent components

**Implementation**:
```python
class PLSRModel:
    def __init__(self, n_components=10):
        self.pls = PLSRegression(n_components=n_components)
    
    def train(self, X_train, y_train):
        return self.pls.fit(X_train, y_train)
    
    def predict(self, X):
        return self.pls.predict(X)
```

**Hyperparameters**:
- `n_components`: 5-15 (number of latent components)

**Best For**: Spectral data with high multicollinearity

---

### 2. GBRT (Gradient Boosting Regression Tree)

**Purpose**: Ensemble method combining weak learners sequentially

**Hyperparameters**:
- `n_estimators`: 50-300 (number of trees)
- `learning_rate`: 0.01-0.2 (shrinkage)
- `max_depth`: 3-10 (tree depth)

**Tuning Strategy**: Bayesian optimization on validation set

---

### 3. KRR (Kernel Ridge Regression)

**Purpose**: Non-linear regression with regularization

**Kernels Tested**:
- RBF (Radial Basis Function)
- Polynomial
- Linear

**Hyperparameters**:
- `alpha`: 0.01-1000 (regularization)
- `kernel`: rbf, poly, linear

---

### 4. SVR (Support Vector Regression)

**Purpose**: Support vector method for regression

**Hyperparameters**:
- `C`: 1-1000 (regularization parameter)
- `epsilon`: 0.01-0.5 (tube margin)
- `kernel`: rbf, poly, linear

---

### 5. Cubist (Ensemble Rule-Based Regression)

**Purpose**: Hybrid ensemble combining gradient boosting rules with k-NN smoothing

**Architecture**:
- **Stage 1**: Gradient Boosting Regressor for rule generation
- **Stage 2**: k-Nearest Neighbors for instance-based smoothing
- **Prediction**: 60% ensemble rules + 40% neighbor averaging

**Mathematical Foundation**:
```python
# Stage 1: Rule generation via ensemble trees
F(x) = F_0(x) + Œ£(ŒΩ ¬∑ h_m(x))  # Gradient boosting

# Stage 2: Instance-based smoothing
y_cubist = 0.6 ¬∑ y_ensemble + 0.4 ¬∑ mean(y_neighbors)
```

**Hyperparameters**:
- `n_estimators`: 80-100 (trees in ensemble)
- `k_neighbors`: 5-7 (neighbors for smoothing)
- `learning_rate`: 0.05-0.1 (gradient boosting shrinkage)
- `max_depth`: 4-5 (tree depth for rules)

**Advantages**:
- Interpretable ensemble rules from gradient boosting
- Instance-based smoothing reduces overfitting
- Hybrid approach balances accuracy and interpretability
- Effective for complex non-linear relationships

**Implementation**: GradientBoostingRegressor + NearestNeighbors (cubist.py)

---

## Data Preprocessing

### SpectralPreprocessor Class

```python
from preprocessing import SpectralPreprocessor

# Initialize
preprocessor = SpectralPreprocessor()

# Fit on training data
preprocessor.fit(
    X_train,
    technique='absorbance',      # spectral transformation
    scaler='standard',            # normalization
    apply_smoothing=False,
    smoothing_window=5
)

# Transform training and test data
X_train_processed = preprocessor.transform(X_train)
X_test_processed = preprocessor.transform(X_test)
```

### Spectral Transformations Explained

**1. Reflectance (Baseline)**
```
R = I_reflected / I_incident
Range: [0, 1]
Use: Direct spectral data
```

**2. Absorbance (Log-transformed)**
```
A = log(1/R)
= -log(R)
Effect: Enhances subtle absorption features
Linear relationship with concentration (Beer-Lambert Law)
```

**3. Continuum Removal**
```
CR = R / continuum(R)
Purpose: Remove background reflectance trends
Result: Normalized features relative to local baseline
```

### Normalization Methods

| Scaler | Formula | Use Case |
|--------|---------|----------|
| StandardScaler | (x - mean)/std | When data ~ Normal distribution |
| MinMaxScaler | (x - min)/(max - min) | Bounded data [0,1] |
| RobustScaler | (x - median)/IQR | Data with outliers |

---

## Feature Implementation

### 1. Dual Paradigm Training

**What**: Standard vs Tuned hyperparameters trained in parallel

**How**:
```python
# Standard Paradigm (fixed parameters)
trainer_standard = ModelTrainer(tune_hyperparameters=False)
results_standard = trainer_standard.train(X_train, y_train, X_test, y_test)

# Tuned Paradigm (Bayesian Optimization)
trainer_tuned = ModelTrainer(tune_hyperparameters=True, cv_folds=5)
results_tuned = trainer_tuned.train(X_train, y_train, X_test, y_test)

# Parallel execution with threading
```

**Impact**: See which paradigm performs better for your data

---

### 2. AI-Powered Insights

**What**: LLM-based analysis using Google Generative AI or OpenAI

**Context Provided**:
- Training results (all 15 model-technique combinations)
- Performance metrics by algorithm
- Consistency analysis (Std Dev across techniques)
- Data characteristics
- Best/worst performing combinations

**Features**:
```python
# Conversational AI
- Ask questions about model performance
- Get recommendations on feature engineering
- Understand why certain models perform better

# Automated Report Generation
- Comparison reports (Standard vs Tuned)
- Summary statistics
- Improvement analysis
- Production recommendations

# Example Question:
"Which model is most consistent across techniques?"
‚Üí AI analyzes Std Dev across all techniques
‚Üí Recommends model with lowest variance
```

---

### 3. Comprehensive Analytics Dashboard

**Tabs in Results View**:

1. **Overview**: KPIs, top models, comparisons
2. **Analytics**: Detailed metrics, filtering, ranking
3. **Model Details**: Per-model performance breakdown
4. **Comparison** (when 2 paradigms): Side-by-side analysis
5. **Reports**: AI-generated insights & recommendations
6. **Export**: Download results as CSV/Markdown

---

### 4. Model Inspection & Visualization

**Available Visualizations**:
- Model performance scatter plots
- Residual plots
- Feature importance (tree-based models)
- Cross-validation score distributions
- Technique comparison bar charts
- Algorithm consistency analysis

---

### 5. Prediction Interface

**After Training**:
- Load saved models
- Input new spectral data
- Get predictions with confidence intervals
- Compare predictions across models
- Export predictions

---

## Cross-Validation Strategy

### Overview

The application supports two configurable cross-validation strategies for robust model evaluation:

1. **K-Fold Cross-Validation** (Default): Standard k-fold with configurable splits
2. **Leave-One-Out Cross-Validation** (LOO): Maximum robustness, one sample held out at a time

### K-Fold CV (Default)

```python
cv_strategy = 'k-fold'
cv_folds = 5  # 5-fold CV (default)

# Process:
# Split training data into 5 folds
# For each fold:
#   ‚îú‚îÄ Train on 4 folds
#   ‚îú‚îÄ Evaluate on 1 fold
#   ‚îî‚îÄ Record metrics
# Average results from all 5 folds
```

**Characteristics**:
- Fast training
- Suitable for larger datasets
- Standard approach, well-studied
- Less computational overhead

### Leave-One-Out CV

```python
cv_strategy = 'leave-one-out'

# Process:
# For each sample in training data:
#   ‚îú‚îÄ Remove sample from training
#   ‚îú‚îÄ Train model on remaining samples
#   ‚îú‚îÄ Predict removed sample
#   ‚îú‚îÄ Evaluate prediction error
#   ‚îî‚îÄ Record metrics
# Average results across all samples
```

**Characteristics**:
- Maximum robustness (each sample validated by model never trained on it)
- Comprehensive but computationally expensive
- Better for smaller datasets
- Generates LOO CV metrics: LOO_CV_Test_R¬≤, LOO_CV_Test_RMSE

### Test Set Evaluation

Both strategies evaluate models on the held-out test set:
- **Test_R¬≤**: Coefficient of determination on test set
- **Test_RMSE**: Root mean squared error on test set
- **Test_MAE**: Mean absolute error on test set

When LOO CV is enabled, additional metrics are computed:
- **LOO_CV_Test_R¬≤**: LOO CV metric computed on test set (superior validation)
- **LOO_CV_Test_RMSE**: LOO CV error on test set

### Feature Importance Prediction Source

When LOO CV is enabled, feature importance analysis automatically uses LOO CV Test predictions:
```python
# Auto-detection in Feature Importance tab
loo_cv_predictions = results_row.get('LOO_CV_Test_Predictions')
if loo_cv_predictions is not None:
    predictions = loo_cv_predictions          # Use LOO CV
    prediction_source = "Leave-One-Out CV"
else:
    predictions = results_row.get('Predictions')  # Use standard
    prediction_source = "Standard Test Set"
```

---

## Hyperparameter Tuning Strategy

### Overview

The application uses **GridSearchCV** and **RandomizedSearchCV** for finding optimal hyperparameters in the Tuned paradigm. Both methods respect the selected CV strategy (K-Fold or LOO).

### Search Methods

#### GridSearchCV (Exhaustive Search)

Evaluates **EVERY** combination in the parameter grid:

```python
search = GridSearchCV(
    base_model,
    param_grid,
    cv=cv_strategy,           # 'k-fold' or LeaveOneOut()
    scoring='r2',
    n_jobs=-1
)
```

**Characteristics**:
- Exhaustive evaluation of all combinations
- Guarantees finding best combination in the grid
- Time complexity: O(|grid| √ó |cv_folds|)
- Example: GBRT 48 combos √ó 5 CV folds = 240 model trainings
- Best for: Small parameter spaces (PLSR, KRR, Cubist)

#### RandomizedSearchCV (Stochastic Search)

Randomly samples `n_iter` combinations from the parameter space:

```python
search = RandomizedSearchCV(
    base_model,
    param_distributions,
    n_iter=20,                # Only evaluate 20 random samples
    cv=cv_strategy,           # 'k-fold' or LeaveOneOut()
    scoring='r2',
    n_jobs=-1,
    random_state=42
)
```

**Characteristics**:
- Random sampling of combinations (much faster)
- Approximates exhaustive search results
- Time complexity: O(n_iter √ó |cv_folds|) - constant, not exponential
- Example: GBRT 20 random combos √ó 5 CV folds = 100 model trainings (10x faster!)
- Best for: Large parameter spaces with diminishing returns

### Search Method Comparison

| Aspect | GridSearchCV | RandomizedSearchCV |
|--------|--------------|-------------------|
| **Search Type** | Exhaustive | Random sampling |
| **Parameter Combos** | ALL (complete) | n_iter samples |
| **Time** | Longer | Faster |
| **GBRT Example** | 48 combos | 20 combos |
| **Guarantee** | Finds best combo | Approximation |
| **Best For** | Small param spaces | Large param spaces |
| **Use In App** | `search_type='grid'` | `search_type='random'` |

### Architecture

```
Standard Paradigm (Fixed Params)
    PLSR(n_components=10)
    GBRT(n_estimators=100, max_depth=5, learning_rate=0.1)
    SVR(kernel='rbf', C=100)
    KRR(kernel='rbf', alpha=1.0)
    Cubist(n_rules=10, neighbors=5)
    ‚Üì
    [Train on full training set]
    ‚Üì
    Results with default hyperparameters

vs

Tuned Paradigm (Optimized via GridSearchCV)
    [For each model]
    ‚îú‚îÄ Define parameter search space
    ‚îú‚îÄ GridSearchCV: Try ALL combinations
    ‚îÇ  ‚îî‚îÄ Example: GBRT has ~2,000+ combinations to test
    ‚îú‚îÄ Or RandomizedSearchCV: Sample randomly (~20 samples)
    ‚îú‚îÄ Cross-validation: Evaluate on 5 folds
    ‚îú‚îÄ Select best parameters
    ‚îî‚îÄ Train final model with best params
    ‚Üì
    Results with optimized hyperparameters
```

### GridSearchCV vs RandomizedSearchCV

| Aspect | GridSearchCV | RandomizedSearchCV |
|--------|--------------|-------------------|
| **Search Type** | Exhaustive | Random sampling |
| **Parameter Combos** | ALL (Complete) | n_iter samples |
| **Time** | Longer | Faster |
| **GBRT Example** | ~2,000 combinations | ~20 combinations |
| **Best For** | Small param spaces | Large param spaces |
| **Use In App** | `search_type='grid'` | `search_type='random'` |

### Implementation Code

```python
# In hyperparameter_tuner.py

class HyperparameterTuner:
    
    # Predefined hyperparameter search spaces (FULL GRID)
    PARAM_GRIDS = {
        'PLSR': {
            'n_components': [3, 5, 8, 10, 12, 15, 20]         # 7 values
        },
        'GBRT': {
            'n_estimators': [50, 100, 150, 200],              # 4 values
            'learning_rate': [0.001, 0.01, 0.05, 0.1],        # 4 values
            'max_depth': [3, 4, 5, 6, 7],                      # 5 values
            'min_samples_split': [2, 5, 10],                   # 3 values
            'min_samples_leaf': [1, 2, 4],                     # 3 values
            'subsample': [0.8, 0.9, 1.0]                       # 3 values
            # Total: 4√ó4√ó5√ó3√ó3√ó3 = 2,160 combinations
        },
        'SVR': {
            'kernel': ['rbf', 'poly', 'linear'],               # 3 values
            'C': [0.1, 1, 10, 100, 1000],                      # 5 values
            'epsilon': [0.01, 0.1, 0.5],                       # 3 values
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]       # 5 values
            # Total: 3√ó5√ó3√ó5 = 225 combinations
        },
        'KRR': {
            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],            # 5 values
            'kernel': ['rbf', 'linear', 'polynomial'],          # 3 values
            'gamma': [None, 0.001, 0.01, 0.1, 1.0]             # 5 values
            # Total: 5√ó3√ó5 = 75 combinations
        },
        'Cubist': {
            'n_rules': [5, 10, 15, 20, 25],                     # 5 values
            'neighbors': [0, 3, 5, 7, 9]                        # 5 values
            # Total: 5√ó5 = 25 combinations
        }
    }
    
    # Smaller grids for faster search (DEFAULT - use_small_grid=True)
    PARAM_GRIDS_SMALL = {
        'PLSR': {
            'n_components': [5, 10, 15]                         # 3 values
            # Total: 3 combinations
        },
        'GBRT': {
            'n_estimators': [100, 150],                         # 2 values
            'learning_rate': [0.01, 0.1],                       # 2 values
            'max_depth': [4, 5, 6],                             # 3 values
            'min_samples_split': [2, 5],                        # 2 values
            'subsample': [0.9, 1.0]                             # 2 values
            # Total: 2√ó2√ó3√ó2√ó2 = 48 combinations (vs 2,160!)
        },
        'SVR': {
            'kernel': ['rbf', 'linear'],                        # 2 values
            'C': [1, 100],                                      # 2 values
            'epsilon': [0.1, 0.5],                              # 2 values
            'gamma': ['scale', 'auto']                          # 2 values
            # Total: 2√ó2√ó2√ó2 = 16 combinations (vs 225!)
        },
        'KRR': {
            'alpha': [0.01, 1.0, 10.0],                         # 3 values
            'kernel': ['rbf', 'linear'],                        # 2 values
            'gamma': [None, 0.01, 0.1]                          # 3 values
            # Total: 3√ó2√ó3 = 18 combinations (vs 75!)
        },
        'Cubist': {
            'n_estimators': [80, 100],                          # 2 values
            'k_neighbors': [5, 7],                              # 2 values
            'learning_rate': [0.05, 0.1],                       # 2 values
            'max_depth': [4, 5]                                 # 2 values
            # Total: 2√ó2√ó2√ó2 = 16 combinations (vs 100+!)
        }
    }
    
    def tune_model(self, base_model, X_train, y_train):
        """Tune model hyperparameters."""
        
        # Select CV strategy
        if self.cv_strategy == 'leave-one-out':
            cv_folds = LeaveOneOut()
        else:
            cv_folds = self.cv_folds  # k-fold (default: 5)
        
        if self.search_type == 'random':
            # RandomizedSearchCV - faster, random sampling
            search = RandomizedSearchCV(
                base_model,
                self.param_grid,
                n_iter=self.n_iter,     # Random samples (default: 20)
                cv=cv_folds,            # K-fold or LOO
                scoring='r2',
                n_jobs=-1,              # Use all cores
                random_state=42
            )
            
        else:
            # GridSearchCV - exhaustive search (default)
            search = GridSearchCV(
                base_model,
                self.param_grid,
                cv=cv_folds,            # K-fold or LOO
                scoring='r2',
                n_jobs=-1,              # Parallel computation
                verbose=1
            )
        
        # Fit - finds best parameters
        search.fit(X_train, y_train)
        
        # Return results
        return {
            'best_params': search.best_params_,
            'best_score': search.best_score_,
            'cv_results': search.cv_results_
        }
```

### Cross-Validation Process

**GridSearchCV Example (GBRT with small grid)**:

```
Step 1: Define parameter space
‚îú‚îÄ n_estimators: [100, 150]
‚îú‚îÄ learning_rate: [0.01, 0.1]
‚îú‚îÄ max_depth: [4, 5, 6]
‚îú‚îÄ min_samples_split: [2, 5]
‚îî‚îÄ subsample: [0.9, 1.0]
   Total combinations: 48

Step 2: For each combination (48 times):
‚îú‚îÄ Create model with these params
‚îú‚îÄ Run 5-fold cross-validation
‚îÇ  ‚îú‚îÄ Split data into 5 folds
‚îÇ  ‚îú‚îÄ Train on 4 folds
‚îÇ  ‚îú‚îÄ Evaluate on 1 fold
‚îÇ  ‚îú‚îÄ Repeat 5 times (different fold as test)
‚îÇ  ‚îî‚îÄ Average 5 scores
‚îî‚îÄ Record CV score

Step 3: Select best combination
‚îî‚îÄ Pick combination with highest average CV score

Step 4: Train final model
‚îú‚îÄ Use best parameters
‚îú‚îÄ Train on entire X_train, y_train
‚îî‚îÄ Return trained model
```

**CV Fold Example (one combination, 5 folds)**:

```python
X_train = [1000 samples]
y_train = [1000 targets]

# 5-fold CV
Fold 1: Train on samples 1-800,   Test on samples 801-1000   ‚Üí R¬≤ = 0.52
Fold 2: Train on samples 1-200+801-1000, Test on 201-800    ‚Üí R¬≤ = 0.48
Fold 3: Train on samples 201-1000, Test on 1-200            ‚Üí R¬≤ = 0.50
Fold 4: Train on samples 1-200+201-600+801-1000, Test 600-800 ‚Üí R¬≤ = 0.51
Fold 5: Train on samples 1-600+801-1000, Test on 601-800    ‚Üí R¬≤ = 0.49

Average CV Score: (0.52 + 0.48 + 0.50 + 0.51 + 0.49) / 5 = 0.50
```

### Computational Cost Example

```
Full Tuning of All 5 Models (with SMALL GRID - default):

PLSR (3 combinations):
‚îú‚îÄ 3 combinations √ó 5 CV folds = 15 model trainings
‚îî‚îÄ Time: ~2 seconds

GBRT (48 combinations):
‚îú‚îÄ 48 combinations √ó 5 CV folds = 240 model trainings
‚îî‚îÄ Time: ~60 seconds

SVR (16 combinations):
‚îú‚îÄ 16 combinations √ó 5 CV folds = 80 model trainings
‚îî‚îÄ Time: ~20 seconds

KRR (18 combinations):
‚îú‚îÄ 18 combinations √ó 5 CV folds = 90 model trainings
‚îî‚îÄ Time: ~12 seconds

Cubist (16 combinations):
‚îú‚îÄ 16 combinations √ó 5 CV folds = 80 model trainings
‚îî‚îÄ Time: ~8 seconds

TOTAL with SMALL GRID: ~122 seconds (~2 minutes) ‚úÖ
       with FULL GRID: ~30-40 minutes (2,160 + 225 + 75 + 100+ = 2,560+ combinations)

---

Comparison of Total Grid Sizes:

| Model | Full Grid | Small Grid | Reduction |
|-------|-----------|-----------|-----------|
| PLSR | 7 | 3 | 57% |
| GBRT | 2,160 | 48 | 98% ‚úÖ |
| SVR | 225 | 16 | 93% ‚úÖ |
| KRR | 75 | 18 | 76% ‚úÖ |
| Cubist | 100+ | 16 | 84%+ ‚úÖ |
| **TOTAL** | **2,567+** | **101** | **96% Reduction** |
```

---

## Implementation Details

### 1. AI Integration (How It Works)

#### Architecture

```
User Question / Training Results
    ‚Üì
[ContextBuilder] - Builds comprehensive context
    ‚îú‚îÄ Training results (all 15 combinations)
    ‚îú‚îÄ Data statistics
    ‚îú‚îÄ Model performance metrics
    ‚îî‚îÄ Historical comparison data
    ‚Üì
[AIExplainer / ExplainerWithCache] - Interfaces with LLM
    ‚îú‚îÄ Detects AI provider (Google Generative AI / OpenAI)
    ‚îú‚îÄ Validates API key
    ‚îú‚îÄ Implements response caching
    ‚îî‚îÄ Handles rate limiting & errors
    ‚Üì
[LLM (Gemini or ChatGPT)] - Generates insights
    ‚îú‚îÄ Analyzes context
    ‚îú‚îÄ Generates recommendations
    ‚îî‚îÄ Answers specific questions
    ‚Üì
[Response] - Formatted result to user
```

#### Implementation Code

**AI Provider Detection** (`ai_explainer.py`):
```python
class AIExplainerFactory:
    @staticmethod
    def create_explainer(provider: str = "gemini", api_key: str = None):
        """Create appropriate AI explainer based on provider."""
        if provider.lower() == "gemini":
            return GoogleAIExplainer(api_key)
        elif provider.lower() == "openai":
            return OpenAIExplainer(api_key)
        else:
            raise ValueError(f"Unknown provider: {provider}")
```

**Response Caching** (improves performance):
```python
class ExplainerWithCache:
    def __init__(self, provider, api_key, cache_enabled=True):
        self.explainer = AIExplainerFactory.create_explainer(provider, api_key)
        self.cache = {} if cache_enabled else None
    
    def generate_insights(self, context):
        # Check cache first
        cache_key = hash(context)
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # Generate if not cached
        response = self.explainer.generate(context)
        self.cache[cache_key] = response
        return response
```

#### Context Building Example

```python
# In services.py - ContextBuilder class
def build_training_context(results_df, raw_data, target_col, paradigm="Standard"):
    """Build comprehensive context for AI."""
    context = f"""
    TRAINING PARADIGM: {paradigm}
    
    DATA OVERVIEW:
    - Samples: {len(raw_data)}
    - Features: {raw_data.shape[1]}
    - Target: {target_col}
    
    TRAINING RESULTS (15 model-technique combinations):
    - Best R¬≤: {results_df['Test_R¬≤'].max():.4f}
    - Mean R¬≤: {results_df['Test_R¬≤'].mean():.4f}
    - Models tested: {', '.join(results_df['Model'].unique())}
    - Techniques used: {', '.join(results_df['Technique'].unique())}
    
    PERFORMANCE BY ALGORITHM:
    {generate_algorithm_performance_section(results_df)}
    
    TOP MODELS:
    {generate_top_models_section(results_df)}
    """
    return context
```

#### Chat Interface Features

```python
class ChatInterface:
    """Conversational interface for model insights."""
    
    def __init__(self, ai_provider="gemini", api_key=None):
        self.explainer = AIExplainerFactory.create_explainer(ai_provider, api_key)
        self.cycle_chat_history = []      # Current session only
        self.persistent_history = []       # Saved to disk
    
    def chat(self, user_query, context):
        """Process user query and get AI response."""
        # Add to histories
        self.add_message('user', user_query)
        
        # Get AI response with context
        response = self.explainer.chat(
            messages=self.cycle_chat_history,
            context=context
        )
        
        # Add response to histories
        self.add_message('assistant', response)
        
        # Save globally
        GlobalChatHistory.add_message('assistant', response)
        
        return response
```

---

### 2. Analytics Implementation

#### Architecture

```
Training Results (15 combinations)
    ‚Üì
[MetricsAnalyzer] - Compute aggregate metrics
    ‚îú‚îÄ Performance by technique
    ‚îú‚îÄ Performance by model
    ‚îú‚îÄ Performance by paradigm
    ‚îî‚îÄ Consistency metrics (Std Dev)
    ‚Üì
[ModelAnalyzer] - Detailed model statistics
    ‚îú‚îÄ Per-model performance breakdown
    ‚îú‚îÄ Hyperparameter values
    ‚îú‚îÄ Cross-validation scores
    ‚îî‚îÄ Technique comparison per model
    ‚Üì
[Visualizations] - Plotly charts
    ‚îú‚îÄ Scatter plots (model vs metrics)
    ‚îú‚îÄ Box plots (distribution by technique)
    ‚îú‚îÄ Bar charts (technique comparison)
    ‚îî‚îÄ Heatmaps (model-technique performance matrix)
```

#### Metrics Calculation

```python
class MetricsAnalyzer:
    @staticmethod
    def compute_metrics_by_technique(results_df):
        """Compute aggregated metrics per technique."""
        by_technique = {}
        
        for technique in results_df['Technique'].unique():
            tech_data = results_df[results_df['Technique'] == technique]
            
            by_technique[technique] = {
                'mean_r2': tech_data['Test_R¬≤'].mean(),
                'std_r2': tech_data['Test_R¬≤'].std(),      # Consistency
                'max_r2': tech_data['Test_R¬≤'].max(),
                'count': len(tech_data),
                'best_model': tech_data.loc[
                    tech_data['Test_R¬≤'].idxmax(), 'Model'
                ]
            }
        
        return by_technique
    
    @staticmethod
    def compute_metrics_by_model(results_df):
        """Compute aggregated metrics per model."""
        by_model = {}
        
        for model in results_df['Model'].unique():
            model_data = results_df[results_df['Model'] == model]
            
            by_model[model] = {
                'mean_r2': model_data['Test_R¬≤'].mean(),
                'std_r2': model_data['Test_R¬≤'].std(),      # Consistency
                'techniques': model_data['Technique'].unique().tolist(),
                'consistency_rank': None  # Filled later
            }
        
        # Rank models by consistency (lower std = more consistent)
        models_sorted = sorted(by_model.items(), 
                              key=lambda x: x[1]['std_r2'])
        for rank, (model, _) in enumerate(models_sorted, 1):
            by_model[model]['consistency_rank'] = rank
        
        return by_model
```

#### Dashboard Rendering

```python
# In interface.py
def render_analytics_dashboard(results_df, paradigm):
    """Render comprehensive analytics dashboard."""
    
    # Create tabs
    tab1, tab2, tab3 = st.tabs([
        "üìä Overview",
        "üîç Detailed Analysis", 
        "üìà Trends"
    ])
    
    with tab1:
        # KPI metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Best R¬≤", f"{results_df['Test_R¬≤'].max():.4f}")
        with col2:
            st.metric("Mean R¬≤", f"{results_df['Test_R¬≤'].mean():.4f}")
        with col3:
            st.metric("Total Models", len(results_df))
        with col4:
            st.metric("Techniques", results_df['Technique'].nunique())
    
    with tab2:
        # Detailed metrics by technique
        metrics_by_tech = MetricsAnalyzer.compute_metrics_by_technique(results_df)
        
        # Display as table
        tech_df = pd.DataFrame(metrics_by_tech).T
        st.dataframe(tech_df, use_container_width=True)
    
    with tab3:
        # Visualizations
        fig = px.box(
            results_df,
            x='Technique',
            y='Test_R¬≤',
            color='Model',
            title='R¬≤ Distribution by Technique and Model'
        )
        st.plotly_chart(fig, use_container_width=True)
```

---

### 3. Report Generation (Step-by-Step)

#### Single Paradigm Report

```
Process Flow:
‚îú‚îÄ User clicks "Generate Report" button
‚îú‚îÄ ReportGenerator initializes
‚îú‚îÄ [Collect Data]
‚îÇ  ‚îú‚îÄ Read results_df (15 rows)
‚îÇ  ‚îú‚îÄ Get raw_data statistics
‚îÇ  ‚îî‚îÄ Extract target column info
‚îú‚îÄ [Generate Summary]
‚îÇ  ‚îú‚îÄ Best/worst/mean/median R¬≤
‚îÇ  ‚îú‚îÄ Std Dev for consistency
‚îÇ  ‚îî‚îÄ Top 5 models
‚îú‚îÄ [Generate Statistics]
‚îÇ  ‚îú‚îÄ By technique breakdown
‚îÇ  ‚îú‚îÄ By model breakdown
‚îÇ  ‚îî‚îÄ Cross-validation scores
‚îú‚îÄ [AI Insights] (if enabled)
‚îÇ  ‚îú‚îÄ Send context to LLM
‚îÇ  ‚îú‚îÄ Receive insights
‚îÇ  ‚îî‚îÄ Cache response
‚îî‚îÄ [Format & Display]
   ‚îú‚îÄ Markdown formatting
   ‚îú‚îÄ Display in UI
   ‚îî‚îÄ Provide download option
```

#### Code Implementation

```python
class ReportGenerator:
    def generate_training_report(self, results_df, paradigm="Standard"):
        """Generate report for single paradigm."""
        
        report = {
            'title': f'{paradigm} Training Report',
            'timestamp': datetime.now().isoformat(),
            'paradigm': paradigm,
            
            # 1. Summary statistics
            'summary': {
                'total_models': len(results_df),
                'best_r2': float(results_df['Test_R¬≤'].max()),
                'mean_r2': float(results_df['Test_R¬≤'].mean()),
                'best_model': results_df.loc[
                    results_df['Test_R¬≤'].idxmax(), 'Model'
                ]
            },
            
            # 2. Detailed statistics
            'statistics': self._calculate_statistics(results_df),
            
            # 3. Top models
            'top_models': self._get_top_models(results_df, top_n=5),
            
            # 4. Technique analysis
            'technique_analysis': self._analyze_techniques(results_df)
        }
        
        # 5. AI insights (if available)
        if self.ai_available:
            context = ContextBuilder.build_training_context(
                results_df, 
                self.raw_data,
                self.target_col,
                paradigm
            )
            report['ai_insights'] = self._generate_ai_insights(
                results_df, 
                paradigm, 
                context
            )
        
        return report
    
    def _generate_ai_insights(self, results_df, paradigm, context):
        """Generate AI-powered insights."""
        prompt = f"""
        Analyze the following {paradigm} training results and provide insights:
        
        Context:
        {context}
        
        Provide:
        1. Key findings and patterns
        2. Why certain models perform better
        3. Recommendations for improvement
        4. Concerns or limitations
        5. Which techniques work best for this data
        """
        
        response = self.explainer.generate_insights(prompt)
        return response
```

#### Comparison Report

```python
def generate_comparison_report(self, standard_results, tuned_results):
    """Generate comparison between paradigms."""
    
    report = {
        'title': 'Standard vs Tuned Paradigm Comparison',
        'timestamp': datetime.now().isoformat(),
        
        # Summaries for both
        'standard_summary': self._generate_summary(standard_results),
        'tuned_summary': self._generate_summary(tuned_results),
        
        # Improvements calculation
        'improvements': {
            'best_r2_improvement': (
                tuned_results['Test_R¬≤'].max() - 
                standard_results['Test_R¬≤'].max()
            ),
            'mean_r2_improvement': (
                tuned_results['Test_R¬≤'].mean() - 
                standard_results['Test_R¬≤'].mean()
            ),
            'rmse_improvement': (
                standard_results['Test_RMSE'].min() - 
                tuned_results['Test_RMSE'].min()
            ),
            'tuning_beneficial': (
                tuned_results['Test_R¬≤'].max() > 
                standard_results['Test_R¬≤'].max()
            )
        },
        
        # Top models for each
        'standard_top_models': self._get_top_models(standard_results),
        'tuned_top_models': self._get_top_models(tuned_results),
    }
    
    # AI recommendations
    if self.ai_available:
        report['ai_recommendations'] = self._generate_recommendations(
            standard_results,
            tuned_results
        )
    
    return report
```

#### Markdown Formatting

```python
def format_report_as_markdown(self, report):
    """Format report as markdown string."""
    
    # For comparison reports
    if 'standard_summary' in report:
        md = f"""
# {report['title']}

**Generated**: {report['timestamp']}

## Summary Comparison

### Standard Paradigm
- **Best R¬≤**: {report['standard_summary']['best_r2']:.4f}
- **Mean R¬≤**: {report['standard_summary']['mean_r2']:.4f}
- **Best Model**: {report['standard_summary']['best_model']}

### Tuned Paradigm
- **Best R¬≤**: {report['tuned_summary']['best_r2']:.4f}
- **Mean R¬≤**: {report['tuned_summary']['mean_r2']:.4f}
- **Best Model**: {report['tuned_summary']['best_model']}

## Performance Improvements
- **Best R¬≤ Improvement**: {report['improvements']['best_r2_improvement']:.4f}
- **Mean R¬≤ Improvement**: {report['improvements']['mean_r2_improvement']:.4f}
- **Tuning Beneficial**: {'Yes' if report['improvements']['tuning_beneficial'] else 'No'}

## Top Models - Standard
{self._format_top_models(report['standard_top_models'])}

## Top Models - Tuned
{self._format_top_models(report['tuned_top_models'])}

## AI Recommendations
{report.get('ai_recommendations', 'N/A')}
"""
    
    return md
```

---

### 4. Streamlit Integration & UI Rendering

#### Session State Management

```python
def init_session_state():
    """Initialize all session state variables."""
    
    # Data state
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'raw_data' not in st.session_state:
        st.session_state.raw_data = None
    if 'target_col' not in st.session_state:
        st.session_state.target_col = None
    
    # Training state
    if 'training_complete' not in st.session_state:
        st.session_state.training_complete = False
    if 'all_results' not in st.session_state:
        st.session_state.all_results = {}
    
    # Report state
    if 'comparison_report' not in st.session_state:
        st.session_state.comparison_report = None
    
    # Chat state
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
```

#### Report Generation in UI

```python
# In interface.py - render_paradigm_results function

with col_report:
    if st.button("üìù Generate Report"):
        try:
            with st.spinner("Generating AI report..."):
                gen = ReportGenerator(
                    raw_data=st.session_state.raw_data,
                    target_col=st.session_state.target_col
                )
                
                # Generate report
                report = gen.generate_training_report(
                    results_df, 
                    paradigm,
                    include_ai_insights=True
                )
                
                # Format as markdown
                report_md = gen.format_report_as_markdown(report)
                
                # Display
                st.markdown(report_md)
                
                # Download option
                st.download_button(
                    "üì• Download Report",
                    report_md,
                    f"report_{paradigm}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    "text/markdown"
                )
                
                st.success("‚úÖ Report generated!")
        
        except Exception as e:
            st.error(f"Error: {str(e)}")
```

---

### 5. User Personas & Workflows

#### Persona 1: Soil Scientist / Researcher

**Goal**: Find best model for soil property prediction

**Workflow**:
1. Load spectral data CSV
2. Select target soil property (e.g., "Soil Moisture")
3. Train both Standard and Tuned paradigms
4. Compare performance metrics
5. Review AI recommendations
6. Ask AI questions: "Which model is most robust?"
7. Download comparison report
8. Use best model for future predictions

**Key Features Used**:
- Analytics dashboard
- Model comparison
- AI chat
- Report generation
- Export functionality

---

#### Persona 2: Data Scientist / ML Engineer

**Goal**: Optimize model performance and understand why tuning helps

**Workflow**:
1. Load data
2. Run Tuned paradigm only (focus on hyperparameter tuning)
3. Inspect individual model hyperparameters
4. View cross-validation scores
5. Check improvement percentages
6. Extract best hyperparameters
7. Export detailed results
8. Deploy best model

**Key Features Used**:
- Training orchestration
- Hyperparameter inspection
- Cross-validation analysis
- Detailed metrics
- Model export

---

#### Persona 3: Domain Expert / Decision Maker

**Goal**: Understand model performance and business impact

**Workflow**:
1. View summary KPIs
2. Read AI-generated recommendations
3. Ask AI: "Was tuning worth the effort?"
4. Review comparison report
5. Check "Is this model ready for production?"
6. View confidence intervals / uncertainty estimates
7. Share report with stakeholders

**Key Features Used**:
- Dashboard KPIs
- AI insights
- AI chat
- Comparison reports
- Export for presentations

---

### 6. Error Handling & Logging

#### Logging Architecture

```python
# In system.py
def initialize_logging(log_dir="./logs"):
    """Initialize comprehensive logging."""
    
    log_dir = Path(log_dir)
    log_dir.mkdir(exist_ok=True)
    
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    
    # File handler
    file_handler = logging.FileHandler(
        log_dir / f"ssm_{datetime.now().strftime('%Y%m%d')}.log"
    )
    file_handler.setLevel(logging.DEBUG)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger
```

#### Exception Handling Pattern

```python
def safe_operation(func_name, fallback_value=None):
    """Decorator for safe operations with error handling."""
    def decorator(func):
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.error(
                    f"Error in {func_name}: {str(e)}", 
                    exc_info=True
                )
                if fallback_value is not None:
                    return fallback_value
                raise
        return wrapper
    return decorator
```

---

## Application Features

### Training Paradigms (Three Options)

The application offers **THREE distinct training paradigms** selectable via radio button in Step 3:

#### 1Ô∏è‚É£ Standard (No Tuning)
**Label in UI**: "Standard (No Tuning)"

```python
# Uses fixed default hyperparameters
# No hyperparameter optimization
# Fastest training option

models = {
    'PLSR': PLSRModel(n_components=10),
    'GBRT': GBRTModel(n_estimators=100, learning_rate=0.1, max_depth=5),
    'SVR': SVRModel(kernel='rbf', C=100.0, epsilon=0.1, gamma='scale'),
    'KRR': KRRModel(alpha=1.0, kernel='rbf', gamma=None),
    'Cubist': CubistModel(n_rules=20, neighbors=5)
}

# Trains 15 models (5 √ó 3 techniques) with defaults
# Training time: ~150-200 seconds
```

**When to Use**:
- Quick baseline comparison
- Limited computational resources
- Testing data pipeline
- Default behavior comparison

---

#### 2Ô∏è‚É£ Tuned Only (with CV)
**Label in UI**: "Tuned Only (with CV)"

```python
# Hyperparameter optimization using GridSearchCV/RandomizedSearchCV
# 5-fold cross-validation for robust tuning
# Finds optimal parameters for each model-technique combo

tuner = HyperparameterTuner(
    model_name='GBRT',
    cv_folds=5,                   # User-configurable: 3-10
    search_type='grid',           # Exhaustive search
    use_small_grid=True,          # DEFAULT: faster with SMALL grid
    n_iter=20                     # For RandomizedSearchCV
)

# Tunes 15 models with optimized parameters
# Training time: ~400-600 seconds
```

**When to Use**:
- Focus on hyperparameter optimization
- Willing to wait for better parameters
- Want to see improvement from tuning
- Analyzing algorithm sensitivity

---

#### 3Ô∏è‚É£ Both (Compare Standard vs Tuned)
**Label in UI**: "Both (Compare Standard vs Tuned)"

```python
# Trains BOTH paradigms in parallel for direct comparison
# Standard paradigm: fixed defaults
# Tuned paradigm: GridSearchCV optimized

# Standard + Tuned trained simultaneously
# User can compare improvements from tuning
# Generates comparison reports

# Training time: ~600-900 seconds
# (roughly sum of both paradigms due to parallel execution)
```

**When to Use**:
- Want to see if tuning helps for your data
- Decision making: Is tuning worth the effort?
- Comparative analysis
- Full evaluation of both approaches

**Output**: Comparison report showing:
- Best R¬≤ (Standard vs Tuned)
- Mean R¬≤ improvement
- RMSE improvement
- Top models for each paradigm
- AI recommendations

---

### Main Modes

#### 1. Training Mode
```
Step 1: Load Data (CSV format)
        ‚Üì
Step 2: Select Target Column
        ‚Üì
Step 3: Choose Application Mode
        - Standard Paradigm Only
        - Tuned Paradigm Only
        - Both (Parallel Training)
        ‚Üì
Step 4: Select Training Options
        - Spectral Technique
        - Normalization Method
        - Cross-validation Folds
        ‚Üì
Step 5: Start Training
        ‚Üì
Step 6: View Results & Analysis
```

#### 2. Prediction Mode
```
Load Trained Model
    ‚Üì
Input Spectral Data
    ‚Üì
Generate Predictions
    ‚Üì
View Results & Comparisons
```

### Export Capabilities

1. **Results Export**: CSV with all metrics
2. **Report Export**: Markdown with analysis
3. **Model Export**: Joblib-serialized models
4. **Prediction Export**: CSV with predictions

---

### UI Components

**Glassmorphic Design**:
- Semi-transparent cards
- Gradient backgrounds
- Smooth animations
- Responsive layout

**Custom Widgets**:
- Progress indicators
- Metric cards
- Interactive tables
- Real-time plots

---

## Setup & Usage

### Installation

```bash
# Clone repository
git clone <repo_url>
cd Team27_Spectral_Soil_Modeler

# Create virtual environment
python -m venv soil

# Activate environment
source soil/bin/activate  # Linux/Mac
# or
soil\Scripts\activate  # Windows

# Install dependencies
pip install -r soil_modeler/requirements.txt

# Set up API keys (optional, for AI features)
export GOOGLE_API_KEY="your_key_here"
export OPENAI_API_KEY="your_key_here"
```

### Running the Application

```bash
cd soil_modeler

# Run training/prediction app
streamlit run src/app.py

# App opens at http://localhost:8501
```

### Data Format

**CSV Input Requirements**:
- First column: Target variable (soil property)
- Remaining columns: Spectral bands (float values)
- Example: 1 target + 2150 bands = 2151 columns

```
Target_Property,Band_1,Band_2,...,Band_2150
0.5,0.123,0.456,...,0.789
0.7,0.234,0.567,...,0.890
...
```

---

## Key Technical Highlights

### Bayesian Optimization

```python
# Used for hyperparameter tuning in "Tuned" paradigm
from skopt import gp_minimize

def objective(params):
    model = build_model(*params)
    score = cross_val_score(model, X_train, y_train, cv=5)
    return -score.mean()  # Minimize

result = gp_minimize(
    objective,
    space=param_space,
    n_calls=100,          # 100 iterations per model
    n_initial_points=10,
    random_state=42
)
```

### Cross-Validation Strategy

- **CV Folds**: 5-fold cross-validation
- **Purpose**: Estimate generalization error
- **Used for**: Hyperparameter tuning, model selection

### Performance Metrics Explained

- **R¬≤ Score**: % variance explained [0, 1]. Higher is better.
- **RMSE**: Root Mean Squared Error. Lower is better.
- **MAE**: Mean Absolute Error. Lower is better.
- **RPD**: Residual Prediction Deviation. Higher is better. (Std/RMSE)

---

## File Descriptions

| File | Purpose |
|------|---------|
| `app.py` | Main Streamlit app, UI orchestration |
| `interface.py` | UI components, dashboards, visualizations |
| `services.py` | Context building, report generation, AI integration |
| `preprocessing.py` | Spectral transformations, normalization |
| `models/model_trainer.py` | Training orchestration, 15 combinations |
| `models/*.py` | Individual model implementations |
| `evaluation.py` | Metrics computation, result analysis |
| `data_loader.py` | CSV loading, data validation |
| `model_analyzer.py` | Performance analysis, comparisons |
| `chat_interface.py` | LLM integration, conversational AI |
| `system.py` | Logging, utilities, persistence |

---

## Performance Summary

**Typical Results on Soil Spectral Data**:
- Best R¬≤ (Standard): 0.45-0.55
- Best R¬≤ (Tuned): 0.48-0.58
- Improvement: 5-15% with tuning
- Most Consistent: PLSR/SVR
- Fastest: PLSR
- Most Interpretable: Cubist

---

## Future Enhancements

- Deep learning models (1D CNN, LSTM)
- Ensemble methods (stacking, blending)
- Feature selection algorithms
- Real-time model retraining
- Mobile app deployment

---

**Version**: 1.0.0  
**Last Updated**: December 2024  
**Team**: Team 27 - Spectral Soil Modeler
